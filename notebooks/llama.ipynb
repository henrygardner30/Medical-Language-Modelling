{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Text Generation with Llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore fine-tuning Llama! The goal is to compare this model to GPT-2 and the paid fine-tuned GPT-3.5-Turbo. \n",
    "\n",
    "Aid from:\\\n",
    "https://huggingface.co/meta-llama/Llama-3.2-3B\\\n",
    "https://github.com/meta-llama/llama\\\n",
    "https://pytorch.org/torchtune/0.3/tutorials/lora_finetune.html\\\n",
    "https://github.com/microsoft/LoRA\\\n",
    "https://huggingface.co/docs/transformers/en/model_doc/llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 24 17:32:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.95                 Driver Version: 551.95         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8              2W /   50W |       8MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     44020      C   ...0_x64__qbz5n2kfra8p0\\python3.12.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# gather all imports\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(file_path):\n",
    "    \"\"\"\n",
    "    Helper function to load in the data into a specific form \n",
    "\n",
    "    @PARAMS:\n",
    "        - file_path -> the file to process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # format data to be just question answer pairs\n",
    "        formatted_data = []\n",
    "        for entry in data:\n",
    "            formatted_data.append({\n",
    "                \"question\": entry[\"Question\"],\n",
    "                \"response\": entry[\"Answer\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"Loaded {len(formatted_data)} Q&A pairs from {file_path}!\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading in file...\\n{e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18749 Q&A pairs from processed_data/train.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/validation.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/test.json!\n",
      "{'question': 'will eating late evening meals increase my cholesterol?', 'response': 'no. it is what you are eating (as well as your genetics) not when you eat it. it depends on the kinds of foods that you eat. make sure that you are eating healthy foods in order to not gain great amount of cholesterol. you have to always watch what you eat in order to have a healthy skin and body. you may check out www. clearclinic. com for great ideas to achieve an acne free skin.'}\n",
      "{'question': 'who is affected by arthritis?', 'response': 'arthritis sufferers include men and women children and adults. approximately 350 million people worldwide have arthritis. nearly 40 million people in the united states are affected by arthritis including over a quarter million children! more than 27 million americans have osteoarthritis. approximately 1. 3 million americans suffer from rheumatoid arthritis. more than half of those with arthritis are under 65 years of age. nearly 60% of americans with arthritis are women.'}\n",
      "{'question': 'can i be pregnant if i had unprotected sex the 4th day of being on the depo?', 'response': 'yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.'}\n"
     ]
    }
   ],
   "source": [
    "# load in formatted data\n",
    "## TRAIN ##\n",
    "train_data = load_and_prepare(\"processed_data/train.json\")\n",
    "\n",
    "## VAL ##\n",
    "val_data = load_and_prepare(\"processed_data/validation.json\")\n",
    "\n",
    "## TEST ##\n",
    "test_data = load_and_prepare(\"processed_data/test.json\")\n",
    "\n",
    "# print out one value of each to make sure it is loaded correctly\n",
    "print(train_data[0])\n",
    "print(val_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA:\n",
    "    \"\"\"Baseline LLAMA class to handle the 3.2-3B model.\"\"\"\n",
    "    def __init__(self, model_id=\"meta-llama/Llama-3.2-3B\"):\n",
    "        \"\"\"\n",
    "        Initializer function to setup LLAMA!\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_id -> the specific llama model to use\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "    def generate_response(self, input_text, max_length=200, num_sequences=1, top_k=50, top_p=0.95, temperature=0.8):\n",
    "        \"\"\"\n",
    "        Generate text based on the user prompt.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - input_text -> user query\n",
    "            - [*]        -> parameters for specifying model output\n",
    "        \"\"\"\n",
    "        # format the prompt\n",
    "        formatted_prompt = (\n",
    "            \"Instructions: Provide a clear and accurate answer to the following question.\\n\"\n",
    "            f\"Question: {input_text}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        \n",
    "        # tokenize input\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).cuda()\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=num_sequences,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2,           \n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "                min_length=30,\n",
    "            )\n",
    "        \n",
    "        # decode and return results\n",
    "        generated_texts = [\n",
    "            self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                .replace(formatted_prompt, \"\")    # Remove the prompt\n",
    "                .strip()                         # Remove leading/trailing whitespace\n",
    "            for output in outputs\n",
    "        ]\n",
    "        \n",
    "        return generated_texts[0] if num_sequences == 1 else generated_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\n",
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLAMA Baseline Results:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo?\n",
      "Response: It is possible, but not likely. The Depo-Provera shot (medroxyprogesterone) prevents pregnancy for three months; therefore it would need to have been given before you were exposed to sperm in order for there to be no risk of becoming pregnant within that time frame. If your first period after receiving this injection was not expected, then the most recent one may still be considered fertile due to its short half-life or length between doses so we cannot say with certainty whether or not an unwanted result has occurred just yet!\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n",
      "\n",
      "Question: what is delta hepatitis?\n",
      "Response: The infection caused by HBV (hepatitis B virus) can be asymptomatic or cause acute liver failure. If not treated properly, it may lead to chronic hepatitis which will further develop into cirrhosis of the liver with loss of functioning tissue, or hepatocellular carcinoma in later stages\n",
      "What are the symptoms of Delta Hepatitis? How do you treat them if any?\n",
      "Symptoms include jaundice, dark urine, light-colored stools, abdominal pain & swelling, fatigue, nausea, fever; mild itching on skin due to bilirubin discharge through sweat glands causing discoloration around eyes called icterus sclerae), vomiting blood as well, confusion etc…\n",
      "There isn’t much treatment available except maintaining cleanliness at home like keeping things germ-free such as hands when going out so bacteria doesn't get spread anymore onto other surfaces especially food preparation area where possible because ger\n",
      "Expected Response: delta hepatitis is caused by a virus that only infects people who already have hepatitis b. the delta hepatitis virus (also known as hepatitis d or hdv) is an rna virus meaning that its genetic material is made up of ribonucleic acid. it is spread through exposure to contaminated blood especially with illicit intravenous drug use and by sexual contact. delta hepatitis can be acquired at the same time as acute hepatitis b. when this happens infected people are quite sick but more than 95% are eventually able to eliminate the viruses from their bodies. people who already have chronic hepatitis b can acquire delta hepatitis as well. this often causes severe inflammation of the liver and the viruses are less likely to be cleared. delta hepatitis makes chronic hepatitis b much worse. it increases the risk of complications especially cirrhosis which occurs in up to two-thirds of patients. there is no vaccine against delta hepatitis. interferon treatment may cause improvement in the hepatitis but relapse is common after therapy is stopped. prevention includes avoiding contaminated needles and practicing safer sex (abstaining or limiting the number of partners using barrier methods of contraception). universal vaccination of newborns with hepatitis b vaccine effectively prevents delta hepatitis because the delta hepatitis virus only causes disease in the presence of hepatitis b virus.\n",
      "\n",
      "\n",
      "Question: who manufactures actos? my mother can’t afford it & sometimes skips her dose. do they offer an assistance program?\n",
      "Response: We manufacture Actos (pioglitazone) at our U.S., Canadian, Mexican and Brazilian plants. However, we have no information on if or how much you may be able to save with their patient-assistance programs.\n",
      "If your physician is willing to prescribe you an alternative drug for treating Type 2 diabetes — one of which has been approved by FDA for use in diabetics like yourself — this might reduce the cost of treatment.\n",
      "Expected Response: the manufacturer of actos is takeda pharmaceuticals. if you go to www. actos. com and click on \"special offers\" you will see three different types of programs to help with the cost of the medication. you are thinking the right way: if she skips her medication she will put herself in greater danger and may end up in the hospital -- which would definitely be a bigger bill. if you need help on locating these types of programs talk to your pharmacist; we are happy to help.\n",
      "\n",
      "\n",
      "Question: can a breast feeding mother take dapakote. my daughter in law suffers from grand mal seizures and is taking seizure medication along with thyroid medication. she had thyroidectomy (thyroid removed 4 years ago and now the doctor has put her on dapakote er to manage her seizures migraine headaches and mood instability she refuses to take it in fear that it will her 3 month old baby boy that she is breast feeding. can the dapakote harm the baby? the doctor says no any input on this?\n",
      "Response: Dapoxetine was not teratogenic when administered during pregnancy at doses of up to several hundred times human exposure, but there were a few cases reports of neonatal withdrawal symptoms or hypotonia if mothers received higher than recommended dosage throughout their pregnancies. So far there are only case studies about breastfeeding women who have taken it, so I cannot say for certain what would happen if she continued\n",
      "Expected Response: hi i found this piece for you should help her to understand things and you. for those who have taken valproate during pregnancy and would like to breastfeed the news is good. only about 5% of valproate in the mother’s blood passes through breast milk however even less actually enters the baby’s bloodstream. further how valproate affects the baby is unknown and most babies breastfed on valproate experience no side effects. it is strongly felt by neurologists and epileptologists that the benefits of breastfeeding largely outweigh the risks of exposing the baby to antiepileptic medications including valproate. as a matter of fact the american academy of neurology and the american epilepsy society both recommend breastfeeding in women with epilepsy. its from this site <link> do hope this helps you both. good luck\n",
      "\n",
      "\n",
      "Question: what should i do when my baby starts teething and cries a lot?\n",
      "Response: In general, most babies will begin cutting their teeth between 4-7 months. The process can be very painful for your child as he or she has not developed enough teeth yet to protect his gums from irritation and infection. You may want to ask your pediatrician about using Tylenol (acetaminophen) to help relieve pain and reduce fever associated with tooth eruption.\n",
      "If you suspect your child is suffering from infant gastroenteritis, please seek medical attention promptly because this condition often requires hospitalization in order to avoid dehydration. Your physician may prescribe antibiotics depending on how severe the symptoms are; however they cannot cure viral infections like these nor prevent future outbreaks.\n",
      "In addition,you might consider giving your baby over-the-counter medicines such as ibuprofen or naproxen sodium instead of tylenol which both work better against inflammation\n",
      "Expected Response: a teething baby often likes to suck chew and gnaw on things. teething rings can help or often i’ll have parents get a washcloth wet then twist it or roll it up and put it in the freezer or fridge and once it’s cold let the baby chew on that. or you can try freezing a mini-bagel and let them chew on it -- but depending on how young they are you have to be careful keeping an eye on baby so that they don’t bite off a big piece. sometimes i’ll have a parent give a teething baby an appropriate dose of acetaminophen at night which helps soothe the baby and minimize the pain so the baby can sleep longer. i will say to be careful with topical solutions; be careful that you don’t overdo them. i’d recommend checking with your physician before you use them. try a refrigerated pacifier or teether. don't store the teether in the freezer because when frozen it can get hard enough to damage a baby's gums. there are a variety of refrigerated teethers including some with plastic handles so your baby's hands won't get cold. if nothing is working and your baby needs relief your doctor might recommend trying an over-the-counter painkiller like acetaminophen. (note: don't give new medicines to a baby without first checking with a doctor. ask the doctor for the proper dosage whenever giving acetaminophen to a child younger than 2. ) your baby is teething when his or her first set of teeth called primary teeth break through the gums. when does teething typically start? teething usually begins around 6 months of age. but it is normal for teething to start at any time between 3 months and 12 months of age. also find a good dental clinic which consists of experienced pediatricians who will help to give your child proper oral care. there are some great dental clinics which consists of services not only in child dentistry but also in other dental areas like cerec restoration dental implants neuromuscular dentistry and many more.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the generator\n",
    "llama = LLAMA()\n",
    "\n",
    "# generate testing examples:\n",
    "questions = [point['question'] for point in test_data[:5]]\n",
    "responses = [point['response'] for point in test_data[:5]]\n",
    "\n",
    "\n",
    "print(\"\\n\\nLLAMA Baseline Results:\")\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {llama.generate_response(question)}\")\n",
    "    print(f\"Expected Response: {response}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the outputs, we definetely have room for improvement! Unlike GPT-2, I will fine-tune using a process called LoRA, which is a much faster and more efficient fine-tuning procedure. I will comapre the results of GPT-2 to see if it improved a similar amount!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_FineTuned:\n",
    "    \"\"\"Class for the fine-tuned version of LLAMA 3.2-3B\"\"\"\n",
    "    def __init__(self, model_id=\"meta-llama/Llama-3.2-3B\"):\n",
    "        \"\"\"\n",
    "        Initializer function for the fine-tuned version of LLAMA.\n",
    "        \"\"\"\n",
    "        # make sure to clear all GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # save the model id        \n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # setyp the quantization\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # load the model with memory optimization\n",
    "        print(\"Loading base model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "            max_memory={0: \"10GB\"}\n",
    "        )\n",
    "        \n",
    "        # create the configuration for the LORA fine-tuning\n",
    "        config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        # prepare everything \n",
    "        print(\"Applying LoRA...\")\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        self.model = get_peft_model(self.model, config)\n",
    "        \n",
    "        # get some insights into the trainable parameters for this model\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "    def train(self, train_data, output_dir=\"llama-lora-medical\", num_epochs=1, batch_size=1, max_length=200):\n",
    "        \"\"\"\n",
    "        Main training function for the model with memory optimization.\n",
    "\n",
    "        @PARAMS:\n",
    "            - train_data -> all training conversations\n",
    "            - output_dir -> where the model will be saved\n",
    "            - [2:*]       -> hyperparameters for tuning\n",
    "        \"\"\"\n",
    "        print(\"Starting training preparation...\")\n",
    "        \n",
    "        # process in chunks to save time\n",
    "        def prepare_batch(batch_items):\n",
    "            texts = [\n",
    "                # using a basic instruction to see how well it does\n",
    "                f\"Instructions: Provide a clear and accurate answer to the following question.\\nQuestion: {item['question']}\\nAnswer: {item['response']}\"\n",
    "                for item in batch_items\n",
    "            ]\n",
    "            \n",
    "            # create the encodings\n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encodings['input_ids'].cuda(),\n",
    "                'attention_mask': encodings['attention_mask'].cuda(),\n",
    "                'labels': encodings['input_ids'].cuda()\n",
    "            }\n",
    "        \n",
    "        # setup the optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-4,\n",
    "            weight_decay=0.01,\n",
    "            eps=1e-7\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training loop...\")\n",
    "        self.model.train()\n",
    "        \n",
    "        # now loop through each epoch - one full pass through the data\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(range(0, len(train_data), batch_size))\n",
    "            \n",
    "            for i in progress_bar:\n",
    "                # clear cache so GPU isn't overloaded\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # prepare the batch processing\n",
    "                batch_end = min(i + batch_size, len(train_data))\n",
    "                batch_data = train_data[i:batch_end]\n",
    "                batch = prepare_batch(batch_data)\n",
    "                \n",
    "                # forward pass\n",
    "                try:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels']\n",
    "                    )\n",
    "                    \n",
    "                    # update values\n",
    "                    loss = outputs.loss\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    # backward pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # make sure to update the progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': loss.item(),\n",
    "                        'gpu_mem': f\"{torch.cuda.memory_allocated()/1024**2:.0f}MB\"\n",
    "                    })\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    # if we are out of memory, try and clear cache and continue, o/w throw error\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print(f\"\\nOOM error at batch {i}. Current memory: {torch.cuda.memory_allocated()/1024**2:.0f}MB\")\n",
    "                        if torch.cuda.memory_allocated() > 0:\n",
    "                            torch.cuda.empty_cache()\n",
    "                            print(f\"Memory after cache clear: {torch.cuda.memory_allocated()/1024**2:.0f}MB\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                # cleanup!!!\n",
    "                del outputs\n",
    "                del loss\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            # calculate the loss\n",
    "            avg_loss = total_loss / (len(train_data) / batch_size)\n",
    "            print(f'\\nEpoch {epoch+1} average loss: {avg_loss:.4f}')\n",
    "            \n",
    "            # save a checkpoint in case of crash!\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            self.model.save_pretrained(f\"{output_dir}/checkpoint-{epoch+1}\")\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "    def load_finetuned(self, model_path):\n",
    "        \"\"\"\n",
    "        Function to load in a fine-tuned model instead of re-training.\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_path -> where the model config values are\n",
    "        \"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # first load the base model\n",
    "            print(\"Loading base model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            \n",
    "            # now load the pre-trained one\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_id,\n",
    "                quantization_config=quantization_config,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                max_memory={0: \"10GB\"}\n",
    "            )\n",
    "            \n",
    "            # prepare for LORA\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            print(\"Loading LoRA config...\")\n",
    "            lora_config = LoraConfig.from_pretrained(model_path)\n",
    "            \n",
    "            # apply LoRA\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            \n",
    "            # load in config values\n",
    "            print(\"Loading weights and fixing state dict...\")\n",
    "            state_dict = load_file(f\"{model_path}/adapter_model.safetensors\")\n",
    "            \n",
    "            # need to convert keys into the proper format\n",
    "            new_state_dict = {}\n",
    "            for key, value in state_dict.items():\n",
    "                if \"lora_A.weight\" in key or \"lora_B.weight\" in key:\n",
    "                    parts = key.split(\".\")\n",
    "                    new_key = f\"{'.'.join(parts[:-1])}.default.{parts[-1]}\"\n",
    "                    new_state_dict[new_key] = value\n",
    "                else:\n",
    "                    new_state_dict[key] = value\n",
    "            \n",
    "            # load the dict\n",
    "            print(\"Loading adapted weights...\")\n",
    "            self.model.load_state_dict(new_state_dict, strict=False)\n",
    "            \n",
    "            # set to eval \n",
    "            self.model.eval()\n",
    "            print(\"Model loaded successfully!\")\n",
    "            return self\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def generate_response(self, question, max_length=200, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Function to get the response of the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=200,\n",
    "            truncation=True\n",
    "        ).to('cuda')\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                min_length=20,\n",
    "                num_return_sequences=1,\n",
    "                temperature=temperature,\n",
    "                top_p=0.85,\n",
    "                top_k=40,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True,\n",
    "                use_cache=True,\n",
    "                num_beams=1\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response.replace(prompt, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep prevention activated for Windows\n",
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...\n",
      "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n",
      "Starting training preparation...\n",
      "Starting training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2344 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "  0%|          | 1/2344 [00:25<16:38:18, 25.56s/it, loss=5.05, gpu_mem=3768MB]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m fine_tuner \u001b[38;5;241m=\u001b[39m LLAMA_FineTuned()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mfine_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 126\u001b[0m, in \u001b[0;36mLLAMA_FineTuned.train\u001b[1;34m(self, train_data, output_dir, num_epochs, batch_size, max_length)\u001b[0m\n\u001b[0;32m    123\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    128\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prevent the computer from sleeping when running!\n",
    "try:\n",
    "    if platform.system() == 'Windows':\n",
    "        # Windows command to prevent sleep\n",
    "        subprocess.Popen(['powercfg', '-change', '-standby-timeout-ac', '0'])\n",
    "        subprocess.Popen(['powercfg', '-change', '-monitor-timeout-ac', '0'])\n",
    "        print(\"Sleep prevention activated for Windows\")\n",
    "    elif platform.system() == 'Darwin':  # macOS\n",
    "        subprocess.Popen(['caffeinate', '-i'])\n",
    "        print(\"Sleep prevention activated for macOS\")\n",
    "    elif platform.system() == 'Linux':\n",
    "        subprocess.Popen(['systemctl', 'mask', 'sleep.target', 'suspend.target', \n",
    "                        'hibernate.target', 'hybrid-sleep.target'])\n",
    "        print(\"Sleep prevention activated for Linux\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not prevent sleep mode: {e}\")\n",
    "\n",
    "# initialize the model\n",
    "fine_tuner = LLAMA_FineTuned()\n",
    "\n",
    "# train\n",
    "fine_tuner.train(\n",
    "    train_data=train_data,\n",
    "    num_epochs=1,\n",
    "    batch_size=8,\n",
    "    max_length=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I re-ran the cell above after the final model was produced. The output you see if the start with the keyboard interupt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Data for Evaluation\n",
    "\n",
    "Now that we have both the baseline and fine-tuned versions of LLAMA, lets gather results in a file that we can evaluate. Due to the long processing times, I am only collecting 20 values each, as that on its own is taking roughly 200 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA...\n",
      "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n",
      "Loading fine-tuned model from llama-lora-medical...\n",
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA config...\n",
      "Loading weights and fixing state dict...\n",
      "Loading adapted weights...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize baseline and fine-tuned\n",
    "llama = LLAMA()\n",
    "fine_tuner = LLAMA_FineTuned()\n",
    "\n",
    "# load in the trained fine-tuned model\n",
    "fine_tuner.load_finetuned(\"llama-lora-medical\")\n",
    "\n",
    "# get the testing testing examples:\n",
    "questions = [point['question'] for point in test_data[:20]]\n",
    "responses = [point['response'] for point in test_data[:20]]\n",
    "\n",
    "\n",
    "# write responses to a result file\n",
    "with open('llama_results.txt', \"w\", encoding='utf-8') as f:\n",
    "    for question,response in zip(questions, responses):\n",
    "        baseline_response = llama.generate_response(question)\n",
    "        finetuned_response = fine_tuner.generate_response(question)\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Baseline Response: {baseline_response}\\n\")\n",
    "        f.write(f\"Fine-tuned Response: {finetuned_response}\\n\")\n",
    "        f.write(f\"\\nExpected Response: {response}\\n\")\n",
    "        # response seperator\n",
    "        f.write(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU memory release\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.reset_peak_memory_stats(i)\n",
    "        torch.cuda.reset_accumulated_memory_stats(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
