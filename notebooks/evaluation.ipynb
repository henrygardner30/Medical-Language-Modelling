{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Performance of Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all imports\n",
    "\n",
    "import bert_score\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import logging as transformers_logging\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the results files to get the model responses. This will include all the necessary information for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(file_path, model_type):\n",
    "    \"\"\"\n",
    "    Function to parse all the result files. I was building these models over time and should have kept them all in the same form, but instead of re-running I will just process like this.\n",
    "    \n",
    "    @PARAMS:\n",
    "        - file_path  -> results file\n",
    "        - model_type -> flag to specify which model to parse\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    \n",
    "    # establish some flags based on the model input\n",
    "    if model_type == \"gpt2_baseline\":\n",
    "        response_label = \"Baseline Response:\"\n",
    "        single_response = True\n",
    "    elif model_type == \"gpt2_finetuned\" or model_type == \"ensemble\":\n",
    "        response_label = \"Answer:\"\n",
    "        single_response = True\n",
    "    elif model_type == \"llama\":\n",
    "        baseline_label = \"Baseline Response:\"\n",
    "        finetuned_label = \"Fine-tuned Response:\"\n",
    "        single_response = False\n",
    "    elif model_type == \"openai\":\n",
    "        baseline_label = \"Baseline Model Response:\"\n",
    "        finetuned_label = \"Fine-tuned Model Response:\"\n",
    "        single_response = False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # read and split file based on the = delimiter\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    raw_entries = content.split('=' * 80)\n",
    "    \n",
    "    for entry in raw_entries:\n",
    "        if not entry.strip():\n",
    "            continue\n",
    "            \n",
    "        lines = entry.strip().split('\\n')\n",
    "        query = \"\"\n",
    "        baseline = \"\"\n",
    "        finetuned = \"\"\n",
    "        response = \"\"\n",
    "        expected = \"\"\n",
    "        \n",
    "        # parse based on flags set above\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Question:\"):\n",
    "                query = line.replace(\"Question:\", \"\").strip()\n",
    "            elif single_response and line.startswith(response_label):\n",
    "                response = line.replace(response_label, \"\").strip()\n",
    "            elif not single_response and line.startswith(baseline_label):\n",
    "                baseline = line.replace(baseline_label, \"\").strip()\n",
    "            elif not single_response and line.startswith(finetuned_label):\n",
    "                finetuned = line.replace(finetuned_label, \"\").strip()\n",
    "            elif line.startswith(\"Expected Response:\"):\n",
    "                expected = line.replace(\"Expected Response:\", \"\").strip()\n",
    "        \n",
    "        # append the values\n",
    "        if query and expected:\n",
    "            if single_response and response:\n",
    "                entries.append((query, response, expected))\n",
    "            elif not single_response and baseline and finetuned:\n",
    "                entries.append((query, baseline, finetuned, expected))\n",
    "                \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first example of reading through each file to make sure it worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Baseline Results First Example:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo?\n",
      "Response: i have a lot of other options.\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n",
      "GPT-2 Fine-Tuned Results First Example:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo? . i had my period the 4 th day of my period. i had a period the 5th day. i was supposed to have my period on the 4 and 5th of this month. i have a period on my 5th. i am not pregnant. i did not have a cramp or cramp-like feeling. i took a test the 4 days after i was on the pill. i'm not sure if it was a test or if i am pregnant.\n",
      "Response: hi its not a good idea to take a test for a period. if you miss a period then your pregnant. but if you are having unprotected sex then you should be ok. good luck\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n",
      "LLAMA Results First Example:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo?\n",
      "Baseline Response: Do you mean birth control? If so, then no. The shot lasts for three months (12 weeks) before it needs to be replaced; this is not enough time for pregnancy to occur.\n",
      "Fine-tuned Response: hi you need to find out what your cycle is then take a test that way if its a regular 28 day cycle or 30/31 day cycle then it would not be possible to get pregnant this soon after having the shot. do hope this helps. good luck ps did you know there are some things you can do when taking the pill to help keep your sperm count down this may help if you think you have missed one or two days and your worried about getting pregnant? do give this a try. <link> just copy link into google to open and read. good luck again. ps why cant you just go back to the depo again instead of worrying about missing pills. you could even just have another baby as soon as you come off the depo but then again its up to you. just make sure you use protection for a few weeks before you\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n",
      "OpenAI Results First Example:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo?\n",
      "Baseline Response: Yes, there is a chance of becoming pregnant if you had unprotected sex on the 4th day of being on the Depo-Provera shot. The Depo-Provera shot is a very effective form of birth control, but it may take up to 7 days to become fully effective. It's always important to use additional protection, such as condoms, during the first week of starting the shot to prevent pregnancy. If you are concerned about being pregnant, it's best to take a pregnancy test or consult with your healthcare provider for further guidance.\n",
      "Fine-tuned Response: that will be a difficult but not impossible task -- getting pregnant yes. first this question is 2 months old -- about the time that you may have had a period. it would be more likely that you are not pregnant. the depo-provera progestin-only shot really disfavors normal cycling and ovulation. second that fourth day should have provided you with some protection against an unintended conception but obviously not enough. if by chance you miss your next period or have some signs of pregnancy such as painful and swollen breasts nausea food cravings and more frequent urination you will need to take a pregnancy test if the last shot was over 12 weeks ago. even if you get pregnant a termination early on should not affect your chances of having children later on when you are ready. don't depend on the depo for total sexual protection if you are really not prepared for an unplanned pregnancy.\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n",
      "Ensemble Results First Example:\n",
      "Question: can i be pregnant if i had unprotected sex the 4th day of being on the depo?\n",
      "Response: hi unprotected sex can always lead to pregnancy but you may be ok because there would still be traces in your body for a few days it may just have worked. but it is now a waiting game so just wait and see if you get your next period. you know sex is for enjoyment to please yourself and not just our the man its our bodies sex is enjoy full. good luck\n",
      "Expected Response: yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT-2 Baseline Results First Example:\")\n",
    "gpt2_baseline_results = parse_results(\"results/gpt2_baseline_results.txt\", \"gpt2_baseline\")\n",
    "print(\"Question:\", gpt2_baseline_results[0][0])\n",
    "print(\"Response:\", gpt2_baseline_results[0][1])\n",
    "print(\"Expected Response:\", gpt2_baseline_results[0][2])\n",
    "print()\n",
    "\n",
    "print(\"GPT-2 Fine-Tuned Results First Example:\")\n",
    "gpt2_fine_tuned_results = parse_results(\"results/gpt2_fine_tuned_results.txt\", \"gpt2_finetuned\")\n",
    "print(\"Question:\", gpt2_fine_tuned_results[0][0])\n",
    "print(\"Response:\", gpt2_fine_tuned_results[0][1])\n",
    "print(\"Expected Response:\", gpt2_fine_tuned_results[0][2])\n",
    "print()\n",
    "\n",
    "print(\"LLAMA Results First Example:\")\n",
    "llama_results = parse_results(\"results/llama_results.txt\", \"llama\")\n",
    "question, baseline, finetuned, expected = llama_results[0]\n",
    "print(\"Question:\", question)\n",
    "print(\"Baseline Response:\", baseline)\n",
    "print(\"Fine-tuned Response:\", finetuned)\n",
    "print(\"Expected Response:\", expected)\n",
    "print()\n",
    "\n",
    "print(\"OpenAI Results First Example:\")\n",
    "openai_results = parse_results(\"results/openai_model_comparison.txt\", \"openai\")\n",
    "question, baseline, finetuned, expected = openai_results[0]\n",
    "print(\"Question:\", question)\n",
    "print(\"Baseline Response:\", baseline)\n",
    "print(\"Fine-tuned Response:\", finetuned)\n",
    "print(\"Expected Response:\", expected)\n",
    "print()\n",
    "\n",
    "print(\"Ensemble Results First Example:\")\n",
    "ensemble_results = parse_results(\"results/ensemble_results.txt\", \"ensemble\")\n",
    "print(\"Question:\", ensemble_results[0][0])\n",
    "print(\"Response:\", ensemble_results[0][1])\n",
    "print(\"Expected Response:\", ensemble_results[0][2])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Class to evaluate a model response against an expected response using semantic and lexical metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        # initialize BERT model for embeddings\n",
    "        self.model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        # initialize ROUGE scorer\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Function to calculate the embeddings of a piece of text based off the bio bert model!\n",
    "\n",
    "        @PARAMS:\n",
    "            - text -> what to calculate embeddings of\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        attention_mask = inputs['attention_mask']\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        \n",
    "        return mean_embeddings.numpy()\n",
    "\n",
    "    def calculate_metrics(self, model_response, expected_response):\n",
    "        \"\"\"\n",
    "        Calculate multiple metrics comparing model response to expected response.\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_response    -> what we are comparing to the expected response\n",
    "            - expected_response -> the testing example actual doctor response\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # tokenize responses\n",
    "        try:\n",
    "            model_tokens = word_tokenize(model_response.lower())\n",
    "            expected_tokens = word_tokenize(expected_response.lower())\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization failed: {e}\")\n",
    "            model_tokens = model_response.lower().split()\n",
    "            expected_tokens = expected_response.lower().split()\n",
    "            \n",
    "        # calculate METEOR score\n",
    "        try:\n",
    "            metrics['meteor'] = meteor_score([expected_tokens], model_tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"METEOR score calculation failed: {e}\")\n",
    "            metrics['meteor'] = 0.0\n",
    "            \n",
    "        # calculate BERTScore\n",
    "        try:\n",
    "            P, R, F1 = bert_score.score([model_response], [expected_response], lang='en')\n",
    "            metrics['bertscore_precision'] = P.item()\n",
    "            metrics['bertscore_recall'] = R.item()\n",
    "            metrics['bertscore_f1'] = F1.item()\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore calculation failed: {e}\")\n",
    "            metrics['bertscore_precision'] = 0.0\n",
    "            metrics['bertscore_recall'] = 0.0\n",
    "            metrics['bertscore_f1'] = 0.0\n",
    "            \n",
    "        # calculate ROUGE scores\n",
    "        try:\n",
    "            rouge_scores = self.rouge_scorer.score(model_response, expected_response)\n",
    "            metrics['rouge1_precision'] = rouge_scores['rouge1'].precision\n",
    "            metrics['rouge1_recall'] = rouge_scores['rouge1'].recall\n",
    "            metrics['rouge1_f1'] = rouge_scores['rouge1'].fmeasure\n",
    "            metrics['rouge2_precision'] = rouge_scores['rouge2'].precision\n",
    "            metrics['rouge2_recall'] = rouge_scores['rouge2'].recall\n",
    "            metrics['rouge2_f1'] = rouge_scores['rouge2'].fmeasure\n",
    "            metrics['rougeL_precision'] = rouge_scores['rougeL'].precision\n",
    "            metrics['rougeL_recall'] = rouge_scores['rougeL'].recall\n",
    "            metrics['rougeL_f1'] = rouge_scores['rougeL'].fmeasure\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE score calculation failed: {e}\")\n",
    "            for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "                metrics[f'{metric}_precision'] = 0.0\n",
    "                metrics[f'{metric}_recall'] = 0.0\n",
    "                metrics[f'{metric}_f1'] = 0.0\n",
    "        \n",
    "        # now calculate cosine similarity with the bio embeddings!!\n",
    "        try:\n",
    "            model_embedding = self.get_embedding(model_response)\n",
    "            expected_embedding = self.get_embedding(expected_response)\n",
    "            similarity = cosine_similarity(model_embedding, expected_embedding)\n",
    "            metrics['cosine_similarity'] = similarity[0][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Cosine similarity calculation failed: {e}\")\n",
    "            metrics['cosine_similarity'] = 0.0\n",
    "            \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 examples of GPT2 Baseline!\n",
      "{'meteor': 0.05976095617529879, 'bertscore_precision': 0.8740603923797607, 'bertscore_recall': 0.8354758024215698, 'bertscore_f1': 0.854332685470581, 'rouge1_precision': 0.043478260869565216, 'rouge1_recall': 0.14285714285714285, 'rouge1_f1': 0.06666666666666667, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.043478260869565216, 'rougeL_recall': 0.14285714285714285, 'rougeL_f1': 0.06666666666666667, 'cosine_similarity': 0.8436091}\n",
      "{'meteor': 0.07518535445866341, 'bertscore_precision': 0.8463988304138184, 'bertscore_recall': 0.8016108274459839, 'bertscore_f1': 0.8233962059020996, 'rouge1_precision': 0.10407239819004525, 'rouge1_recall': 0.3770491803278688, 'rouge1_f1': 0.1631205673758865, 'rouge2_precision': 0.04090909090909091, 'rouge2_recall': 0.15, 'rouge2_f1': 0.06428571428571428, 'rougeL_precision': 0.08597285067873303, 'rougeL_recall': 0.3114754098360656, 'rougeL_f1': 0.13475177304964536, 'cosine_similarity': 0.9213505}\n",
      "{'meteor': 0.08658149247526938, 'bertscore_precision': 0.8470909595489502, 'bertscore_recall': 0.8241634964942932, 'bertscore_f1': 0.8354700207710266, 'rouge1_precision': 0.09302325581395349, 'rouge1_recall': 0.23529411764705882, 'rouge1_f1': 0.13333333333333333, 'rouge2_precision': 0.011764705882352941, 'rouge2_recall': 0.030303030303030304, 'rouge2_f1': 0.01694915254237288, 'rougeL_precision': 0.06976744186046512, 'rougeL_recall': 0.17647058823529413, 'rougeL_f1': 0.1, 'cosine_similarity': 0.9277596}\n",
      "{'meteor': 0.00758150113722517, 'bertscore_precision': 0.8408003449440002, 'bertscore_recall': 0.7763093709945679, 'bertscore_f1': 0.8072688579559326, 'rouge1_precision': 0.007462686567164179, 'rouge1_recall': 0.3333333333333333, 'rouge1_f1': 0.014598540145985401, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.007462686567164179, 'rougeL_recall': 0.3333333333333333, 'rougeL_f1': 0.014598540145985401, 'cosine_similarity': 0.7624335}\n",
      "{'meteor': 0.050085861476817393, 'bertscore_precision': 0.8508610725402832, 'bertscore_recall': 0.7860451340675354, 'bertscore_f1': 0.8171698451042175, 'rouge1_precision': 0.08123249299719888, 'rouge1_recall': 0.6744186046511628, 'rouge1_f1': 0.14500000000000002, 'rouge2_precision': 0.019662921348314606, 'rouge2_recall': 0.16666666666666666, 'rouge2_f1': 0.035175879396984924, 'rougeL_precision': 0.04201680672268908, 'rougeL_recall': 0.3488372093023256, 'rougeL_f1': 0.07500000000000001, 'cosine_similarity': 0.90494865}\n",
      "{'meteor': 0.11353885315104154, 'bertscore_precision': 0.8423314094543457, 'bertscore_recall': 0.8268029689788818, 'bertscore_f1': 0.834494948387146, 'rouge1_precision': 0.10638297872340426, 'rouge1_recall': 0.17543859649122806, 'rouge1_f1': 0.13245033112582782, 'rouge2_precision': 0.010752688172043012, 'rouge2_recall': 0.017857142857142856, 'rouge2_f1': 0.013422818791946308, 'rougeL_precision': 0.05319148936170213, 'rougeL_recall': 0.08771929824561403, 'rougeL_f1': 0.06622516556291391, 'cosine_similarity': 0.91253203}\n",
      "{'meteor': 0.1395332636711947, 'bertscore_precision': 0.8642891645431519, 'bertscore_recall': 0.8253145217895508, 'bertscore_f1': 0.8443523645401001, 'rouge1_precision': 0.19047619047619047, 'rouge1_recall': 0.46153846153846156, 'rouge1_f1': 0.2696629213483146, 'rouge2_precision': 0.048, 'rouge2_recall': 0.11764705882352941, 'rouge2_f1': 0.06818181818181818, 'rougeL_precision': 0.10317460317460317, 'rougeL_recall': 0.25, 'rougeL_f1': 0.14606741573033707, 'cosine_similarity': 0.9541769}\n",
      "{'meteor': 0.12194629766905528, 'bertscore_precision': 0.8321390748023987, 'bertscore_recall': 0.8080064058303833, 'bertscore_f1': 0.8198952078819275, 'rouge1_precision': 0.1417910447761194, 'rouge1_recall': 0.25675675675675674, 'rouge1_f1': 0.18269230769230765, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.08208955223880597, 'rougeL_recall': 0.14864864864864866, 'rougeL_f1': 0.10576923076923078, 'cosine_similarity': 0.92876416}\n",
      "{'meteor': 0.038167938931297704, 'bertscore_precision': 0.8473344445228577, 'bertscore_recall': 0.8557215929031372, 'bertscore_f1': 0.8515073657035828, 'rouge1_precision': 0.08333333333333333, 'rouge1_recall': 0.25, 'rouge1_f1': 0.125, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.08333333333333333, 'rougeL_recall': 0.25, 'rougeL_f1': 0.125, 'cosine_similarity': 0.8034094}\n",
      "{'meteor': 0.006570302233902759, 'bertscore_precision': 0.8449796438217163, 'bertscore_recall': 0.7994197607040405, 'bertscore_f1': 0.8215685486793518, 'rouge1_precision': 0.0, 'rouge1_recall': 0.0, 'rouge1_f1': 0.0, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.0, 'rougeL_recall': 0.0, 'rougeL_f1': 0.0, 'cosine_similarity': 0.77214265}\n",
      "\n",
      "First 10 examples of GPT2 Baseline!\n",
      "{'meteor': 0.24527744982290434, 'bertscore_precision': 0.8457076549530029, 'bertscore_recall': 0.8510936498641968, 'bertscore_f1': 0.8483920693397522, 'rouge1_precision': 0.2608695652173913, 'rouge1_recall': 0.17142857142857143, 'rouge1_f1': 0.20689655172413796, 'rouge2_precision': 0.045454545454545456, 'rouge2_recall': 0.029411764705882353, 'rouge2_f1': 0.03571428571428571, 'rougeL_precision': 0.17391304347826086, 'rougeL_recall': 0.11428571428571428, 'rougeL_f1': 0.13793103448275862, 'cosine_similarity': 0.9265413}\n",
      "{'meteor': 0.08826388888888889, 'bertscore_precision': 0.9202837944030762, 'bertscore_recall': 0.814318060874939, 'bertscore_f1': 0.8640642166137695, 'rouge1_precision': 0.11764705882352941, 'rouge1_recall': 0.7878787878787878, 'rouge1_f1': 0.20472440944881887, 'rouge2_precision': 0.05454545454545454, 'rouge2_recall': 0.375, 'rouge2_f1': 0.09523809523809523, 'rougeL_precision': 0.08597285067873303, 'rougeL_recall': 0.5757575757575758, 'rougeL_f1': 0.1496062992125984, 'cosine_similarity': 0.92962337}\n",
      "{'meteor': 0.1302369565002129, 'bertscore_precision': 0.8539832830429077, 'bertscore_recall': 0.828355610370636, 'bertscore_f1': 0.8409742712974548, 'rouge1_precision': 0.1744186046511628, 'rouge1_recall': 0.375, 'rouge1_f1': 0.23809523809523814, 'rouge2_precision': 0.023529411764705882, 'rouge2_recall': 0.05128205128205128, 'rouge2_f1': 0.03225806451612903, 'rougeL_precision': 0.12790697674418605, 'rougeL_recall': 0.275, 'rougeL_f1': 0.17460317460317462, 'cosine_similarity': 0.94044566}\n",
      "{'meteor': 0.0832214765100671, 'bertscore_precision': 0.8619133234024048, 'bertscore_recall': 0.8027721047401428, 'bertscore_f1': 0.8312921524047852, 'rouge1_precision': 0.08955223880597014, 'rouge1_recall': 0.46153846153846156, 'rouge1_f1': 0.15, 'rouge2_precision': 0.015037593984962405, 'rouge2_recall': 0.08, 'rouge2_f1': 0.02531645569620253, 'rougeL_precision': 0.07462686567164178, 'rougeL_recall': 0.38461538461538464, 'rougeL_f1': 0.12499999999999999, 'cosine_similarity': 0.90922993}\n",
      "{'meteor': 0.11833365369797458, 'bertscore_precision': 0.8426996469497681, 'bertscore_recall': 0.8147832155227661, 'bertscore_f1': 0.828506350517273, 'rouge1_precision': 0.19047619047619047, 'rouge1_recall': 0.5, 'rouge1_f1': 0.27586206896551724, 'rouge2_precision': 0.03089887640449438, 'rouge2_recall': 0.08148148148148149, 'rouge2_f1': 0.044806517311608965, 'rougeL_precision': 0.08403361344537816, 'rougeL_recall': 0.22058823529411764, 'rougeL_f1': 0.12170385395537525, 'cosine_similarity': 0.9508563}\n",
      "{'meteor': 0.17311608961303465, 'bertscore_precision': 0.8522197008132935, 'bertscore_recall': 0.8305447101593018, 'bertscore_f1': 0.8412426114082336, 'rouge1_precision': 0.23404255319148937, 'rouge1_recall': 0.2894736842105263, 'rouge1_f1': 0.2588235294117647, 'rouge2_precision': 0.021505376344086023, 'rouge2_recall': 0.02666666666666667, 'rouge2_f1': 0.023809523809523815, 'rougeL_precision': 0.14893617021276595, 'rougeL_recall': 0.18421052631578946, 'rougeL_f1': 0.16470588235294117, 'cosine_similarity': 0.93636477}\n",
      "{'meteor': 0.01639344262295082, 'bertscore_precision': 0.8696942925453186, 'bertscore_recall': 0.7954241037368774, 'bertscore_f1': 0.8309028148651123, 'rouge1_precision': 0.023809523809523808, 'rouge1_recall': 0.75, 'rouge1_f1': 0.04615384615384615, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.015873015873015872, 'rougeL_recall': 0.5, 'rougeL_f1': 0.03076923076923077, 'cosine_similarity': 0.84167004}\n",
      "{'meteor': 0.06139677666922486, 'bertscore_precision': 0.8422663807868958, 'bertscore_recall': 0.8087446093559265, 'bertscore_f1': 0.8251651525497437, 'rouge1_precision': 0.1044776119402985, 'rouge1_recall': 0.4117647058823529, 'rouge1_f1': 0.16666666666666669, 'rouge2_precision': 0.007518796992481203, 'rouge2_recall': 0.030303030303030304, 'rouge2_f1': 0.012048192771084336, 'rougeL_precision': 0.05970149253731343, 'rougeL_recall': 0.23529411764705882, 'rougeL_f1': 0.09523809523809523, 'cosine_similarity': 0.9295535}\n",
      "{'meteor': 0.2227544910179641, 'bertscore_precision': 0.8304235935211182, 'bertscore_recall': 0.8703579902648926, 'bertscore_f1': 0.8499219417572021, 'rouge1_precision': 0.25, 'rouge1_recall': 0.07692307692307693, 'rouge1_f1': 0.11764705882352941, 'rouge2_precision': 0.09090909090909091, 'rouge2_recall': 0.02631578947368421, 'rouge2_f1': 0.04081632653061224, 'rougeL_precision': 0.16666666666666666, 'rougeL_recall': 0.05128205128205128, 'rougeL_f1': 0.07843137254901959, 'cosine_similarity': 0.8528097}\n",
      "{'meteor': 0.14423076923076922, 'bertscore_precision': 0.8427965044975281, 'bertscore_recall': 0.8243525624275208, 'bertscore_f1': 0.833472490310669, 'rouge1_precision': 0.18421052631578946, 'rouge1_recall': 0.2, 'rouge1_f1': 0.1917808219178082, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.09210526315789473, 'rougeL_recall': 0.1, 'rougeL_f1': 0.0958904109589041, 'cosine_similarity': 0.91961944}\n",
      "\n",
      "First 10 examples of LLAMA Baseline!\n",
      "{'meteor': 0.261647951142632, 'bertscore_precision': 0.8532255291938782, 'bertscore_recall': 0.8630614280700684, 'bertscore_f1': 0.8581153154373169, 'rouge1_precision': 0.34782608695652173, 'rouge1_recall': 0.25, 'rouge1_f1': 0.2909090909090909, 'rouge2_precision': 0.045454545454545456, 'rouge2_recall': 0.03225806451612903, 'rouge2_f1': 0.03773584905660377, 'rougeL_precision': 0.2608695652173913, 'rougeL_recall': 0.1875, 'rougeL_f1': 0.21818181818181817, 'cosine_similarity': 0.9233377}\n",
      "{'meteor': 0.08916323731138545, 'bertscore_precision': 0.8529953956604004, 'bertscore_recall': 0.8131601810455322, 'bertscore_f1': 0.8326015472412109, 'rouge1_precision': 0.14479638009049775, 'rouge1_recall': 0.6037735849056604, 'rouge1_f1': 0.23357664233576644, 'rouge2_precision': 0.03636363636363636, 'rouge2_recall': 0.15384615384615385, 'rouge2_f1': 0.0588235294117647, 'rougeL_precision': 0.07692307692307693, 'rougeL_recall': 0.32075471698113206, 'rougeL_f1': 0.12408759124087591, 'cosine_similarity': 0.94658566}\n",
      "{'meteor': 0.12194503735325508, 'bertscore_precision': 0.8365046381950378, 'bertscore_recall': 0.8543643951416016, 'bertscore_f1': 0.8453401923179626, 'rouge1_precision': 0.1744186046511628, 'rouge1_recall': 0.24193548387096775, 'rouge1_f1': 0.20270270270270274, 'rouge2_precision': 0.023529411764705882, 'rouge2_recall': 0.03278688524590164, 'rouge2_f1': 0.0273972602739726, 'rougeL_precision': 0.10465116279069768, 'rougeL_recall': 0.14516129032258066, 'rougeL_f1': 0.12162162162162163, 'cosine_similarity': 0.95201695}\n",
      "{'meteor': 0.08625961833275127, 'bertscore_precision': 0.8257640600204468, 'bertscore_recall': 0.8149803876876831, 'bertscore_f1': 0.8203367590904236, 'rouge1_precision': 0.1417910447761194, 'rouge1_recall': 0.27941176470588236, 'rouge1_f1': 0.18811881188118812, 'rouge2_precision': 0.007518796992481203, 'rouge2_recall': 0.014925373134328358, 'rouge2_f1': 0.009999999999999998, 'rougeL_precision': 0.06716417910447761, 'rougeL_recall': 0.1323529411764706, 'rougeL_f1': 0.0891089108910891, 'cosine_similarity': 0.9469795}\n",
      "{'meteor': 0.028552692424218262, 'bertscore_precision': 0.8567923903465271, 'bertscore_recall': 0.7809659838676453, 'bertscore_f1': 0.8171238899230957, 'rouge1_precision': 0.04481792717086835, 'rouge1_recall': 0.6666666666666666, 'rouge1_f1': 0.08398950131233596, 'rouge2_precision': 0.0056179775280898875, 'rouge2_recall': 0.08695652173913043, 'rouge2_f1': 0.010554089709762533, 'rougeL_precision': 0.028011204481792718, 'rougeL_recall': 0.4166666666666667, 'rougeL_f1': 0.05249343832020997, 'cosine_similarity': 0.90974665}\n",
      "{'meteor': 0.03285870755750274, 'bertscore_precision': 0.8344519138336182, 'bertscore_recall': 0.8091878294944763, 'bertscore_f1': 0.8216257095336914, 'rouge1_precision': 0.0425531914893617, 'rouge1_recall': 0.36363636363636365, 'rouge1_f1': 0.07619047619047618, 'rouge2_precision': 0.010752688172043012, 'rouge2_recall': 0.1, 'rouge2_f1': 0.01941747572815534, 'rougeL_precision': 0.02127659574468085, 'rougeL_recall': 0.18181818181818182, 'rougeL_f1': 0.03809523809523809, 'cosine_similarity': 0.8011054}\n",
      "{'meteor': 0.08634222919937205, 'bertscore_precision': 0.8380173444747925, 'bertscore_recall': 0.8252074718475342, 'bertscore_f1': 0.8315630555152893, 'rouge1_precision': 0.10317460317460317, 'rouge1_recall': 0.26, 'rouge1_f1': 0.14772727272727273, 'rouge2_precision': 0.008, 'rouge2_recall': 0.02040816326530612, 'rouge2_f1': 0.011494252873563218, 'rougeL_precision': 0.03968253968253968, 'rougeL_recall': 0.1, 'rougeL_f1': 0.056818181818181816, 'cosine_similarity': 0.94404966}\n",
      "{'meteor': 0.027301092043681748, 'bertscore_precision': 0.8472867012023926, 'bertscore_recall': 0.7928215265274048, 'bertscore_f1': 0.8191497921943665, 'rouge1_precision': 0.029850746268656716, 'rouge1_recall': 0.36363636363636365, 'rouge1_f1': 0.055172413793103454, 'rouge2_precision': 0.007518796992481203, 'rouge2_recall': 0.1, 'rouge2_f1': 0.013986013986013986, 'rougeL_precision': 0.029850746268656716, 'rougeL_recall': 0.36363636363636365, 'rougeL_f1': 0.055172413793103454, 'cosine_similarity': 0.7569097}\n",
      "{'meteor': 0.11904761904761905, 'bertscore_precision': 0.7927883863449097, 'bertscore_recall': 0.873724102973938, 'bertscore_f1': 0.8312909007072449, 'rouge1_precision': 0.3333333333333333, 'rouge1_recall': 0.025157232704402517, 'rouge1_f1': 0.046783625730994156, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.3333333333333333, 'rougeL_recall': 0.025157232704402517, 'rougeL_f1': 0.046783625730994156, 'cosine_similarity': 0.825735}\n",
      "{'meteor': 0.03901170351105332, 'bertscore_precision': 0.8435481786727905, 'bertscore_recall': 0.806310772895813, 'bertscore_f1': 0.8245092630386353, 'rouge1_precision': 0.05263157894736842, 'rouge1_recall': 0.36363636363636365, 'rouge1_f1': 0.09195402298850575, 'rouge2_precision': 0.013333333333333334, 'rouge2_recall': 0.1, 'rouge2_f1': 0.023529411764705882, 'rougeL_precision': 0.039473684210526314, 'rougeL_recall': 0.2727272727272727, 'rougeL_f1': 0.06896551724137931, 'cosine_similarity': 0.79764843}\n",
      "{'meteor': 0.05997818006069781, 'bertscore_precision': 0.8242560625076294, 'bertscore_recall': 0.7867991328239441, 'bertscore_f1': 0.8050921559333801, 'rouge1_precision': 0.08287292817679558, 'rouge1_recall': 0.3488372093023256, 'rouge1_f1': 0.13392857142857142, 'rouge2_precision': 0.011111111111111112, 'rouge2_recall': 0.047619047619047616, 'rouge2_f1': 0.018018018018018018, 'rougeL_precision': 0.03314917127071823, 'rougeL_recall': 0.13953488372093023, 'rougeL_f1': 0.05357142857142857, 'cosine_similarity': 0.87317985}\n",
      "\n",
      "First 10 examples of LLAMA Fine-Tuned!\n",
      "{'meteor': 0.3092457420924574, 'bertscore_precision': 0.8032733201980591, 'bertscore_recall': 0.8607165813446045, 'bertscore_f1': 0.8310034871101379, 'rouge1_precision': 0.6956521739130435, 'rouge1_recall': 0.10062893081761007, 'rouge1_f1': 0.17582417582417584, 'rouge2_precision': 0.18181818181818182, 'rouge2_recall': 0.02531645569620253, 'rouge2_f1': 0.044444444444444446, 'rougeL_precision': 0.34782608695652173, 'rougeL_recall': 0.050314465408805034, 'rougeL_f1': 0.08791208791208792, 'cosine_similarity': 0.9233526}\n",
      "{'meteor': 0.22717372020816562, 'bertscore_precision': 0.8727847337722778, 'bertscore_recall': 0.848389208316803, 'bertscore_f1': 0.8604140877723694, 'rouge1_precision': 0.334841628959276, 'rouge1_recall': 0.5826771653543307, 'rouge1_f1': 0.425287356321839, 'rouge2_precision': 0.10454545454545454, 'rouge2_recall': 0.18253968253968253, 'rouge2_f1': 0.1329479768786127, 'rougeL_precision': 0.18099547511312217, 'rougeL_recall': 0.31496062992125984, 'rougeL_f1': 0.22988505747126434, 'cosine_similarity': 0.9675672}\n",
      "{'meteor': 0.14331210191082802, 'bertscore_precision': 0.8392452001571655, 'bertscore_recall': 0.8480886220932007, 'bertscore_f1': 0.8436437845230103, 'rouge1_precision': 0.20930232558139536, 'rouge1_recall': 0.2571428571428571, 'rouge1_f1': 0.23076923076923075, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.10465116279069768, 'rougeL_recall': 0.12857142857142856, 'rougeL_f1': 0.11538461538461538, 'cosine_similarity': 0.9548317}\n",
      "{'meteor': 0.17421844083893942, 'bertscore_precision': 0.8356664180755615, 'bertscore_recall': 0.8088331818580627, 'bertscore_f1': 0.8220309019088745, 'rouge1_precision': 0.208955223880597, 'rouge1_recall': 0.35443037974683544, 'rouge1_f1': 0.2629107981220657, 'rouge2_precision': 0.03759398496240601, 'rouge2_recall': 0.0641025641025641, 'rouge2_f1': 0.04739336492890995, 'rougeL_precision': 0.1044776119402985, 'rougeL_recall': 0.17721518987341772, 'rougeL_f1': 0.13145539906103285, 'cosine_similarity': 0.9297389}\n",
      "{'meteor': 0.15752361654210945, 'bertscore_precision': 0.8612603545188904, 'bertscore_recall': 0.8301545977592468, 'bertscore_f1': 0.8454214334487915, 'rouge1_precision': 0.25770308123249297, 'rouge1_recall': 0.5786163522012578, 'rouge1_f1': 0.3565891472868217, 'rouge2_precision': 0.0449438202247191, 'rouge2_recall': 0.10126582278481013, 'rouge2_f1': 0.06225680933852141, 'rougeL_precision': 0.09243697478991597, 'rougeL_recall': 0.20754716981132076, 'rougeL_f1': 0.12790697674418605, 'cosine_similarity': 0.957096}\n",
      "{'meteor': 0.2280574647021955, 'bertscore_precision': 0.847192108631134, 'bertscore_recall': 0.8403991460800171, 'bertscore_f1': 0.8437818884849548, 'rouge1_precision': 0.3404255319148936, 'rouge1_recall': 0.32989690721649484, 'rouge1_f1': 0.3350785340314136, 'rouge2_precision': 0.053763440860215055, 'rouge2_recall': 0.052083333333333336, 'rouge2_f1': 0.05291005291005291, 'rougeL_precision': 0.1595744680851064, 'rougeL_recall': 0.15463917525773196, 'rougeL_f1': 0.15706806282722513, 'cosine_similarity': 0.94805026}\n",
      "{'meteor': 0.16778523489932884, 'bertscore_precision': 0.8564995527267456, 'bertscore_recall': 0.8522416353225708, 'bertscore_f1': 0.8543652892112732, 'rouge1_precision': 0.2857142857142857, 'rouge1_recall': 0.3130434782608696, 'rouge1_f1': 0.2987551867219917, 'rouge2_precision': 0.024, 'rouge2_recall': 0.02631578947368421, 'rouge2_f1': 0.02510460251046025, 'rougeL_precision': 0.1349206349206349, 'rougeL_recall': 0.14782608695652175, 'rougeL_f1': 0.14107883817427386, 'cosine_similarity': 0.96662974}\n",
      "{'meteor': 0.14193084708827475, 'bertscore_precision': 0.8296948671340942, 'bertscore_recall': 0.8081680536270142, 'bertscore_f1': 0.8187900185585022, 'rouge1_precision': 0.17164179104477612, 'rouge1_recall': 0.23711340206185566, 'rouge1_f1': 0.19913419913419914, 'rouge2_precision': 0.015037593984962405, 'rouge2_recall': 0.020833333333333332, 'rouge2_f1': 0.017467248908296946, 'rougeL_precision': 0.08208955223880597, 'rougeL_recall': 0.1134020618556701, 'rougeL_f1': 0.09523809523809525, 'cosine_similarity': 0.9539477}\n",
      "{'meteor': 0.2150636039169031, 'bertscore_precision': 0.8132895827293396, 'bertscore_recall': 0.8617494702339172, 'bertscore_f1': 0.836818516254425, 'rouge1_precision': 0.4166666666666667, 'rouge1_recall': 0.054945054945054944, 'rouge1_f1': 0.09708737864077671, 'rouge2_precision': 0.09090909090909091, 'rouge2_recall': 0.011111111111111112, 'rouge2_f1': 0.019801980198019802, 'rougeL_precision': 0.4166666666666667, 'rougeL_recall': 0.054945054945054944, 'rougeL_f1': 0.09708737864077671, 'cosine_similarity': 0.79646987}\n",
      "{'meteor': 0.16055045871559634, 'bertscore_precision': 0.8170598745346069, 'bertscore_recall': 0.830203652381897, 'bertscore_f1': 0.8235793113708496, 'rouge1_precision': 0.2631578947368421, 'rouge1_recall': 0.18691588785046728, 'rouge1_f1': 0.2185792349726776, 'rouge2_precision': 0.02666666666666667, 'rouge2_recall': 0.018867924528301886, 'rouge2_f1': 0.022099447513812154, 'rougeL_precision': 0.14473684210526316, 'rougeL_recall': 0.102803738317757, 'rougeL_f1': 0.12021857923497267, 'cosine_similarity': 0.9309871}\n",
      "{'meteor': 0.07661272311164713, 'bertscore_precision': 0.833358883857727, 'bertscore_recall': 0.7891592383384705, 'bertscore_f1': 0.8106570243835449, 'rouge1_precision': 0.08287292817679558, 'rouge1_recall': 0.45454545454545453, 'rouge1_f1': 0.14018691588785046, 'rouge2_precision': 0.022222222222222223, 'rouge2_recall': 0.125, 'rouge2_f1': 0.03773584905660377, 'rougeL_precision': 0.055248618784530384, 'rougeL_recall': 0.30303030303030304, 'rougeL_f1': 0.09345794392523363, 'cosine_similarity': 0.8879216}\n",
      "\n",
      "First 10 examples of OPENAI Baseline!\n",
      "{'meteor': 0.28818298865373787, 'bertscore_precision': 0.8344534039497375, 'bertscore_recall': 0.874389111995697, 'bertscore_f1': 0.8539546132087708, 'rouge1_precision': 0.6086956521739131, 'rouge1_recall': 0.14893617021276595, 'rouge1_f1': 0.2393162393162393, 'rouge2_precision': 0.09090909090909091, 'rouge2_recall': 0.021505376344086023, 'rouge2_f1': 0.03478260869565217, 'rougeL_precision': 0.34782608695652173, 'rougeL_recall': 0.0851063829787234, 'rougeL_f1': 0.13675213675213674, 'cosine_similarity': 0.9085616}\n",
      "{'meteor': 0.16675420499625015, 'bertscore_precision': 0.8959733843803406, 'bertscore_recall': 0.8290200233459473, 'bertscore_f1': 0.861197292804718, 'rouge1_precision': 0.22624434389140272, 'rouge1_recall': 0.8333333333333334, 'rouge1_f1': 0.3558718861209964, 'rouge2_precision': 0.1, 'rouge2_recall': 0.3728813559322034, 'rouge2_f1': 0.15770609318996417, 'rougeL_precision': 0.11764705882352941, 'rougeL_recall': 0.43333333333333335, 'rougeL_f1': 0.18505338078291814, 'cosine_similarity': 0.9465208}\n",
      "{'meteor': 0.16823702945559083, 'bertscore_precision': 0.8763521313667297, 'bertscore_recall': 0.8561235666275024, 'bertscore_f1': 0.8661197423934937, 'rouge1_precision': 0.2441860465116279, 'rouge1_recall': 0.3, 'rouge1_f1': 0.26923076923076916, 'rouge2_precision': 0.07058823529411765, 'rouge2_recall': 0.08695652173913043, 'rouge2_f1': 0.07792207792207792, 'rougeL_precision': 0.16279069767441862, 'rougeL_recall': 0.2, 'rougeL_f1': 0.17948717948717952, 'cosine_similarity': 0.9490256}\n",
      "{'meteor': 0.05087235996326906, 'bertscore_precision': 0.8037196397781372, 'bertscore_recall': 0.7972582578659058, 'bertscore_f1': 0.8004758954048157, 'rouge1_precision': 0.06716417910447761, 'rouge1_recall': 0.3, 'rouge1_f1': 0.10975609756097561, 'rouge2_precision': 0.007518796992481203, 'rouge2_recall': 0.034482758620689655, 'rouge2_f1': 0.012345679012345678, 'rougeL_precision': 0.05223880597014925, 'rougeL_recall': 0.23333333333333334, 'rougeL_f1': 0.08536585365853659, 'cosine_similarity': 0.9028508}\n",
      "{'meteor': 0.03145863385118144, 'bertscore_precision': 0.8799089193344116, 'bertscore_recall': 0.7948160171508789, 'bertscore_f1': 0.8352006673812866, 'rouge1_precision': 0.047619047619047616, 'rouge1_recall': 0.7391304347826086, 'rouge1_f1': 0.0894736842105263, 'rouge2_precision': 0.014044943820224719, 'rouge2_recall': 0.22727272727272727, 'rouge2_f1': 0.026455026455026454, 'rougeL_precision': 0.028011204481792718, 'rougeL_recall': 0.43478260869565216, 'rougeL_f1': 0.05263157894736842, 'cosine_similarity': 0.89005}\n",
      "{'meteor': 0.09493670886075949, 'bertscore_precision': 0.8531482219696045, 'bertscore_recall': 0.8307116031646729, 'bertscore_f1': 0.8417804837226868, 'rouge1_precision': 0.1276595744680851, 'rouge1_recall': 0.26666666666666666, 'rouge1_f1': 0.17266187050359708, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.07446808510638298, 'rougeL_recall': 0.15555555555555556, 'rougeL_f1': 0.10071942446043165, 'cosine_similarity': 0.9471437}\n",
      "{'meteor': 0.17617242759288212, 'bertscore_precision': 0.8729320764541626, 'bertscore_recall': 0.8566005229949951, 'bertscore_f1': 0.8646892309188843, 'rouge1_precision': 0.29365079365079366, 'rouge1_recall': 0.3592233009708738, 'rouge1_f1': 0.3231441048034934, 'rouge2_precision': 0.016, 'rouge2_recall': 0.0196078431372549, 'rouge2_f1': 0.01762114537444934, 'rougeL_precision': 0.15873015873015872, 'rougeL_recall': 0.1941747572815534, 'rougeL_f1': 0.1746724890829694, 'cosine_similarity': 0.96125007}\n",
      "{'meteor': 0.10305343511450381, 'bertscore_precision': 0.8769656419754028, 'bertscore_recall': 0.8222323060035706, 'bertscore_f1': 0.8487173914909363, 'rouge1_precision': 0.15671641791044777, 'rouge1_recall': 0.5526315789473685, 'rouge1_f1': 0.24418604651162792, 'rouge2_precision': 0.007518796992481203, 'rouge2_recall': 0.02702702702702703, 'rouge2_f1': 0.011764705882352941, 'rougeL_precision': 0.08955223880597014, 'rougeL_recall': 0.3157894736842105, 'rougeL_f1': 0.13953488372093023, 'cosine_similarity': 0.93075544}\n",
      "{'meteor': 0.33501310219713765, 'bertscore_precision': 0.8393963575363159, 'bertscore_recall': 0.9127871990203857, 'bertscore_f1': 0.8745547533035278, 'rouge1_precision': 0.75, 'rouge1_recall': 0.1232876712328767, 'rouge1_f1': 0.2117647058823529, 'rouge2_precision': 0.2727272727272727, 'rouge2_recall': 0.041666666666666664, 'rouge2_f1': 0.07228915662650602, 'rougeL_precision': 0.6666666666666666, 'rougeL_recall': 0.1095890410958904, 'rougeL_f1': 0.18823529411764706, 'cosine_similarity': 0.9075717}\n",
      "{'meteor': 0.23048327137546468, 'bertscore_precision': 0.883658230304718, 'bertscore_recall': 0.8647307753562927, 'bertscore_f1': 0.8740920424461365, 'rouge1_precision': 0.27631578947368424, 'rouge1_recall': 0.4772727272727273, 'rouge1_f1': 0.35000000000000003, 'rouge2_precision': 0.10666666666666667, 'rouge2_recall': 0.18604651162790697, 'rouge2_f1': 0.13559322033898305, 'rougeL_precision': 0.18421052631578946, 'rougeL_recall': 0.3181818181818182, 'rougeL_f1': 0.23333333333333334, 'cosine_similarity': 0.9607524}\n",
      "{'meteor': 0.09572072072072071, 'bertscore_precision': 0.8227500915527344, 'bertscore_recall': 0.7907660007476807, 'bertscore_f1': 0.806441068649292, 'rouge1_precision': 0.143646408839779, 'rouge1_recall': 0.32098765432098764, 'rouge1_f1': 0.1984732824427481, 'rouge2_precision': 0.005555555555555556, 'rouge2_recall': 0.0125, 'rouge2_f1': 0.007692307692307692, 'rougeL_precision': 0.07734806629834254, 'rougeL_recall': 0.1728395061728395, 'rougeL_f1': 0.10687022900763356, 'cosine_similarity': 0.81415236}\n",
      "\n",
      "First 10 examples of OPENAI Fine-Tuned!\n",
      "{'meteor': 0.3241018629674092, 'bertscore_precision': 0.8030959963798523, 'bertscore_recall': 0.859291672706604, 'bertscore_f1': 0.8302440047264099, 'rouge1_precision': 0.6521739130434783, 'rouge1_recall': 0.09554140127388536, 'rouge1_f1': 0.16666666666666669, 'rouge2_precision': 0.09090909090909091, 'rouge2_recall': 0.01282051282051282, 'rouge2_f1': 0.02247191011235955, 'rougeL_precision': 0.391304347826087, 'rougeL_recall': 0.05732484076433121, 'rougeL_f1': 0.1, 'cosine_similarity': 0.9218545}\n",
      "{'meteor': 0.2952727552602266, 'bertscore_precision': 0.8387335538864136, 'bertscore_recall': 0.8496699333190918, 'bertscore_f1': 0.8441662788391113, 'rouge1_precision': 0.5565610859728507, 'rouge1_recall': 0.40594059405940597, 'rouge1_f1': 0.46946564885496184, 'rouge2_precision': 0.12272727272727273, 'rouge2_recall': 0.08940397350993377, 'rouge2_f1': 0.10344827586206896, 'rougeL_precision': 0.21266968325791855, 'rougeL_recall': 0.1551155115511551, 'rougeL_f1': 0.1793893129770992, 'cosine_similarity': 0.98094535}\n",
      "{'meteor': 0.164330492835432, 'bertscore_precision': 0.8469675183296204, 'bertscore_recall': 0.8540046215057373, 'bertscore_f1': 0.850471556186676, 'rouge1_precision': 0.22093023255813954, 'rouge1_recall': 0.3275862068965517, 'rouge1_f1': 0.26388888888888884, 'rouge2_precision': 0.023529411764705882, 'rouge2_recall': 0.03508771929824561, 'rouge2_f1': 0.028169014084507043, 'rougeL_precision': 0.08139534883720931, 'rougeL_recall': 0.1206896551724138, 'rougeL_f1': 0.09722222222222224, 'cosine_similarity': 0.95107234}\n",
      "{'meteor': 0.2623354637805278, 'bertscore_precision': 0.7932191491127014, 'bertscore_recall': 0.8467568159103394, 'bertscore_f1': 0.8191140294075012, 'rouge1_precision': 0.5373134328358209, 'rouge1_recall': 0.15031315240083507, 'rouge1_f1': 0.23491027732463296, 'rouge2_precision': 0.09022556390977443, 'rouge2_recall': 0.02510460251046025, 'rouge2_f1': 0.03927986906710311, 'rougeL_precision': 0.2537313432835821, 'rougeL_recall': 0.0709812108559499, 'rougeL_f1': 0.11092985318107668, 'cosine_similarity': 0.9389827}\n",
      "{'meteor': 0.037324145851277635, 'bertscore_precision': 0.8424001336097717, 'bertscore_recall': 0.7814455628395081, 'bertscore_f1': 0.810778796672821, 'rouge1_precision': 0.06162464985994398, 'rouge1_recall': 0.6285714285714286, 'rouge1_f1': 0.11224489795918367, 'rouge2_precision': 0.0056179775280898875, 'rouge2_recall': 0.058823529411764705, 'rouge2_f1': 0.010256410256410256, 'rougeL_precision': 0.036414565826330535, 'rougeL_recall': 0.37142857142857144, 'rougeL_f1': 0.0663265306122449, 'cosine_similarity': 0.904333}\n",
      "{'meteor': 0.016519823788546252, 'bertscore_precision': 0.8575524687767029, 'bertscore_recall': 0.8063055276870728, 'bertscore_f1': 0.8311398029327393, 'rouge1_precision': 0.02127659574468085, 'rouge1_recall': 0.2857142857142857, 'rouge1_f1': 0.039603960396039604, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.02127659574468085, 'rougeL_recall': 0.2857142857142857, 'rougeL_f1': 0.039603960396039604, 'cosine_similarity': 0.8881158}\n",
      "{'meteor': 0.20594893555899196, 'bertscore_precision': 0.7821007966995239, 'bertscore_recall': 0.8175901174545288, 'bertscore_f1': 0.7994517683982849, 'rouge1_precision': 0.35714285714285715, 'rouge1_recall': 0.16917293233082706, 'rouge1_f1': 0.22959183673469388, 'rouge2_precision': 0.04, 'rouge2_recall': 0.018867924528301886, 'rouge2_f1': 0.025641025641025637, 'rougeL_precision': 0.19047619047619047, 'rougeL_recall': 0.09022556390977443, 'rougeL_f1': 0.12244897959183673, 'cosine_similarity': 0.93056476}\n",
      "{'meteor': 0.21041285085048714, 'bertscore_precision': 0.7970505356788635, 'bertscore_recall': 0.8267894983291626, 'bertscore_f1': 0.8116476535797119, 'rouge1_precision': 0.41044776119402987, 'rouge1_recall': 0.1791530944625407, 'rouge1_f1': 0.24943310657596368, 'rouge2_precision': 0.05263157894736842, 'rouge2_recall': 0.02287581699346405, 'rouge2_f1': 0.03189066059225512, 'rougeL_precision': 0.1791044776119403, 'rougeL_recall': 0.0781758957654723, 'rougeL_f1': 0.10884353741496597, 'cosine_similarity': 0.94179404}\n",
      "{'meteor': 0.07575757575757576, 'bertscore_precision': 0.8502920866012573, 'bertscore_recall': 0.8637546300888062, 'bertscore_f1': 0.856970489025116, 'rouge1_precision': 0.08333333333333333, 'rouge1_recall': 0.2, 'rouge1_f1': 0.11764705882352941, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.08333333333333333, 'rougeL_recall': 0.2, 'rougeL_f1': 0.11764705882352941, 'cosine_similarity': 0.8878154}\n",
      "{'meteor': 0.14268727705112957, 'bertscore_precision': 0.8333539962768555, 'bertscore_recall': 0.8267570734024048, 'bertscore_f1': 0.8300424218177795, 'rouge1_precision': 0.19736842105263158, 'rouge1_recall': 0.19230769230769232, 'rouge1_f1': 0.19480519480519481, 'rouge2_precision': 0.013333333333333334, 'rouge2_recall': 0.012987012987012988, 'rouge2_f1': 0.013157894736842108, 'rougeL_precision': 0.09210526315789473, 'rougeL_recall': 0.08974358974358974, 'rougeL_f1': 0.09090909090909091, 'cosine_similarity': 0.92721796}\n",
      "{'meteor': 0.17994649432684895, 'bertscore_precision': 0.7982863187789917, 'bertscore_recall': 0.8023850917816162, 'bertscore_f1': 0.8003304600715637, 'rouge1_precision': 0.281767955801105, 'rouge1_recall': 0.24401913875598086, 'rouge1_f1': 0.26153846153846155, 'rouge2_precision': 0.022222222222222223, 'rouge2_recall': 0.019230769230769232, 'rouge2_f1': 0.020618556701030927, 'rougeL_precision': 0.13259668508287292, 'rougeL_recall': 0.11483253588516747, 'rougeL_f1': 0.12307692307692308, 'cosine_similarity': 0.9175359}\n",
      "\n",
      "First 10 examples of the Ensemble Model!\n",
      "{'meteor': 0.25078369905956116, 'bertscore_precision': 0.8204691410064697, 'bertscore_recall': 0.8551268577575684, 'bertscore_f1': 0.8374395966529846, 'rouge1_precision': 0.391304347826087, 'rouge1_recall': 0.1232876712328767, 'rouge1_f1': 0.1875, 'rouge2_precision': 0.045454545454545456, 'rouge2_recall': 0.013888888888888888, 'rouge2_f1': 0.02127659574468085, 'rougeL_precision': 0.21739130434782608, 'rougeL_recall': 0.0684931506849315, 'rougeL_f1': 0.10416666666666664, 'cosine_similarity': 0.92669237}\n",
      "{'meteor': 0.29467070600677053, 'bertscore_precision': 0.8070517778396606, 'bertscore_recall': 0.8422381281852722, 'bertscore_f1': 0.8242696523666382, 'rouge1_precision': 0.6923076923076923, 'rouge1_recall': 0.17936694021101993, 'rouge1_f1': 0.2849162011173184, 'rouge2_precision': 0.15454545454545454, 'rouge2_recall': 0.03990610328638498, 'rouge2_f1': 0.06343283582089553, 'rougeL_precision': 0.2986425339366516, 'rougeL_recall': 0.07737397420867527, 'rougeL_f1': 0.12290502793296092, 'cosine_similarity': 0.97118455}\n",
      "{'meteor': 0.18430425851372756, 'bertscore_precision': 0.8507629632949829, 'bertscore_recall': 0.8570175170898438, 'bertscore_f1': 0.8538787961006165, 'rouge1_precision': 0.27906976744186046, 'rouge1_recall': 0.34285714285714286, 'rouge1_f1': 0.3076923076923077, 'rouge2_precision': 0.09411764705882353, 'rouge2_recall': 0.11594202898550725, 'rouge2_f1': 0.10389610389610389, 'rougeL_precision': 0.1744186046511628, 'rougeL_recall': 0.21428571428571427, 'rougeL_f1': 0.19230769230769232, 'cosine_similarity': 0.947929}\n",
      "{'meteor': 0.19362222222222225, 'bertscore_precision': 0.8341062068939209, 'bertscore_recall': 0.816977858543396, 'bertscore_f1': 0.8254532217979431, 'rouge1_precision': 0.3358208955223881, 'rouge1_recall': 0.391304347826087, 'rouge1_f1': 0.3614457831325301, 'rouge2_precision': 0.045112781954887216, 'rouge2_recall': 0.05263157894736842, 'rouge2_f1': 0.048582995951417, 'rougeL_precision': 0.13432835820895522, 'rougeL_recall': 0.1565217391304348, 'rougeL_f1': 0.14457831325301204, 'cosine_similarity': 0.9638599}\n",
      "{'meteor': 0.24609524203560917, 'bertscore_precision': 0.8138489127159119, 'bertscore_recall': 0.8165351152420044, 'bertscore_f1': 0.8151898384094238, 'rouge1_precision': 0.3949579831932773, 'rouge1_recall': 0.43653250773993807, 'rouge1_f1': 0.41470588235294115, 'rouge2_precision': 0.05898876404494382, 'rouge2_recall': 0.06521739130434782, 'rouge2_f1': 0.06194690265486726, 'rougeL_precision': 0.13445378151260504, 'rougeL_recall': 0.14860681114551083, 'rougeL_f1': 0.1411764705882353, 'cosine_similarity': 0.9569382}\n",
      "{'meteor': 0.20491803278688525, 'bertscore_precision': 0.801292896270752, 'bertscore_recall': 0.8283900022506714, 'bertscore_f1': 0.8146161437034607, 'rouge1_precision': 0.3829787234042553, 'rouge1_recall': 0.19889502762430938, 'rouge1_f1': 0.2618181818181818, 'rouge2_precision': 0.043010752688172046, 'rouge2_recall': 0.022222222222222223, 'rouge2_f1': 0.029304029304029304, 'rougeL_precision': 0.1595744680851064, 'rougeL_recall': 0.08287292817679558, 'rougeL_f1': 0.10909090909090909, 'cosine_similarity': 0.9334675}\n",
      "{'meteor': 0.23241838429921838, 'bertscore_precision': 0.806049644947052, 'bertscore_recall': 0.8259763717651367, 'bertscore_f1': 0.8158913850784302, 'rouge1_precision': 0.49206349206349204, 'rouge1_recall': 0.19935691318327975, 'rouge1_f1': 0.2837528604118993, 'rouge2_precision': 0.048, 'rouge2_recall': 0.01935483870967742, 'rouge2_f1': 0.02758620689655172, 'rougeL_precision': 0.25396825396825395, 'rougeL_recall': 0.10289389067524116, 'rougeL_f1': 0.14645308924485126, 'cosine_similarity': 0.9420172}\n",
      "{'meteor': 0.061255742725880545, 'bertscore_precision': 0.8407933115959167, 'bertscore_recall': 0.8120822906494141, 'bertscore_f1': 0.8261884450912476, 'rouge1_precision': 0.04477611940298507, 'rouge1_recall': 0.17647058823529413, 'rouge1_f1': 0.07142857142857144, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.029850746268656716, 'rougeL_recall': 0.11764705882352941, 'rougeL_f1': 0.047619047619047616, 'cosine_similarity': 0.9449129}\n",
      "{'meteor': 0.078125, 'bertscore_precision': 0.9527206420898438, 'bertscore_recall': 0.8553597927093506, 'bertscore_f1': 0.901418924331665, 'rouge1_precision': 0.08333333333333333, 'rouge1_recall': 1.0, 'rouge1_f1': 0.15384615384615385, 'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_f1': 0.0, 'rougeL_precision': 0.08333333333333333, 'rougeL_recall': 1.0, 'rougeL_f1': 0.15384615384615385, 'cosine_similarity': 0.7506551}\n",
      "{'meteor': 0.09423336171420463, 'bertscore_precision': 0.8246277570724487, 'bertscore_recall': 0.8127597570419312, 'bertscore_f1': 0.8186507225036621, 'rouge1_precision': 0.13157894736842105, 'rouge1_recall': 0.37037037037037035, 'rouge1_f1': 0.1941747572815534, 'rouge2_precision': 0.013333333333333334, 'rouge2_recall': 0.038461538461538464, 'rouge2_f1': 0.019801980198019806, 'rougeL_precision': 0.07894736842105263, 'rougeL_recall': 0.2222222222222222, 'rougeL_f1': 0.11650485436893203, 'cosine_similarity': 0.89963424}\n"
     ]
    }
   ],
   "source": [
    "# initialize the evaluator to run on each of the testing examples!\n",
    "evaluator = Evaluator()\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "# GPT2 BASELINE!\n",
    "gpt2_baseline_metrics = []\n",
    "with open(\"GPT_BASELINE_METRICS.txt\", \"w\") as f:\n",
    "    for index in range(len(gpt2_baseline_results)):\n",
    "        #                                                 model response             expected response\n",
    "        result = evaluator.calculate_metrics(gpt2_baseline_results[index][1], gpt2_baseline_results[index][2])\n",
    "        gpt2_baseline_metrics.append(result)\n",
    "        f.write(f\"{result}\\n\")\n",
    "print(\"First 10 examples of GPT2 Baseline!\")\n",
    "for i in gpt2_baseline_metrics[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "# GPT2 FINE-TUNED!\n",
    "gpt2_fine_tuned_metrics = []\n",
    "with open(\"GPT_FINETUNED_METRICS.txt\", \"w\") as f:\n",
    "    for index in range(len(gpt2_fine_tuned_results)):\n",
    "        #                                                 model response             expected response\n",
    "        result = evaluator.calculate_metrics(gpt2_fine_tuned_results[index][1], gpt2_fine_tuned_results[index][2])\n",
    "        gpt2_fine_tuned_metrics.append(result)\n",
    "        f.write(f\"{result}\\n\")\n",
    "print(\"First 10 examples of GPT2 Baseline!\")\n",
    "for i in gpt2_fine_tuned_metrics[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "# LLAMA RESULTS!\n",
    "llama_baseline_metrics = []\n",
    "llama_fine_tuned_metrics = []\n",
    "\n",
    "\n",
    "for index in range(len(llama_results)):\n",
    "    question, baseline, finetuned, expected = llama_results[index]\n",
    "    #                                                   model response  expected response\n",
    "    llama_baseline_metrics.append(evaluator.calculate_metrics(baseline, expected))\n",
    "    llama_fine_tuned_metrics.append(evaluator.calculate_metrics(finetuned, expected))\n",
    "print(\"First 10 examples of LLAMA Baseline!\")\n",
    "with open(\"LLAMA_BASELINE_METRICS.txt\", \"w\") as f:\n",
    "    for index, value in enumerate(llama_baseline_metrics):\n",
    "        if index < 11:\n",
    "            print(value)\n",
    "        f.write(f\"{value}\\n\")\n",
    "print(\"\\nFirst 10 examples of LLAMA Fine-Tuned!\")\n",
    "with open(\"LLAMA_FINETUNED_METRICS.txt\", \"w\") as f:\n",
    "    for index, value in enumerate(llama_fine_tuned_metrics):\n",
    "        if index < 11:\n",
    "            print(value)\n",
    "        f.write(f\"{value}\\n\")\n",
    "print()\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "# OPENAI RESULTS!\n",
    "openai_baseline_metrics = []\n",
    "openai_fine_tuned_metrics = []\n",
    "\n",
    "for index in range(len(openai_results)):\n",
    "    question, baseline, finetuned, expected = openai_results[index]\n",
    "    #                                                   model response  expected response\n",
    "    openai_baseline_metrics.append(evaluator.calculate_metrics(baseline, expected))\n",
    "    openai_fine_tuned_metrics.append(evaluator.calculate_metrics(finetuned, expected))\n",
    "print(\"First 10 examples of OPENAI Baseline!\")\n",
    "with open(\"OPENAI_BASELINE_METRICS.txt\", \"w\") as f:\n",
    "    for index, value in enumerate(openai_baseline_metrics):\n",
    "        if index < 11:\n",
    "            print(value)\n",
    "        f.write(f\"{value}\\n\")\n",
    "print(\"\\nFirst 10 examples of OPENAI Fine-Tuned!\")\n",
    "with open(\"OPENAI_FINETUNED_METRICS.txt\", \"w\") as f:\n",
    "    for index, value in enumerate(openai_fine_tuned_metrics):\n",
    "        if index < 11:\n",
    "            print(value)\n",
    "        f.write(f\"{value}\\n\")\n",
    "print()\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "# ENSEMBLE RESULTS!\n",
    "ensemble_metrics = []\n",
    "\n",
    "with open(\"ENSEMBLE_METRICS.txt\", \"w\") as f:\n",
    "    for index in range(len(ensemble_results)):\n",
    "        #                                         model response             expected response\n",
    "        result = evaluator.calculate_metrics(ensemble_results[index][1], ensemble_results[index][2])\n",
    "        ensemble_metrics.append(result)\n",
    "        f.write(f\"{result}\\n\")\n",
    "print(\"First 10 examples of the Ensemble Model!\")\n",
    "for i in ensemble_metrics[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now that we have all the metrics, lets write a quick function to parse it and compute some basic statsitics to get the important insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metrics(data_list):\n",
    "    \"\"\"\n",
    "    Function to take a list of dictionary data values and compute metrics for each of the keys.\n",
    "    \n",
    "    @PARAMS:\n",
    "        - data -> all metric values\n",
    "    \"\"\"\n",
    "    # convert list of dictionaries to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # value to store results\n",
    "    results = {}\n",
    "    \n",
    "    # compute statistics for each metric\n",
    "    for column in df.columns:\n",
    "        stats = {\n",
    "            'mean': df[column].mean(),\n",
    "            'median': df[column].median(),\n",
    "            'std': df[column].std(),\n",
    "            'min': df[column].min(),\n",
    "            'max': df[column].max(),\n",
    "            'q1': df[column].quantile(0.25),\n",
    "            'q3': df[column].quantile(0.75),\n",
    "            'iqr': df[column].quantile(0.75) - df[column].quantile(0.25),\n",
    "            'count': df[column].count()\n",
    "        }\n",
    "        # append results of the column\n",
    "        results[column] = stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_analysis(results) -> None:\n",
    "    \"\"\"\n",
    "    Function to print the results of the stats of the metrics!\n",
    "    \n",
    "    @PARAMS:\n",
    "        - results -> data with the stats of the metrics\n",
    "    \"\"\"\n",
    "    for metric, stats in results.items():\n",
    "        print(f\"\\n=== {metric} ===\")\n",
    "        for stat_name, value in stats.items():\n",
    "            # round to 4 decimal places\n",
    "            print(f\"{stat_name}: {round(value, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.0933\n",
      "median: 0.083\n",
      "std: 0.069\n",
      "min: 0.0\n",
      "max: 0.605\n",
      "q1: 0.0386\n",
      "q3: 0.1335\n",
      "iqr: 0.0948\n",
      "count: 2340\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8514\n",
      "median: 0.8503\n",
      "std: 0.0271\n",
      "min: 0.6886\n",
      "max: 0.99\n",
      "q1: 0.8357\n",
      "q3: 0.8648\n",
      "iqr: 0.0291\n",
      "count: 2340\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8176\n",
      "median: 0.8174\n",
      "std: 0.028\n",
      "min: 0.7043\n",
      "max: 0.99\n",
      "q1: 0.8001\n",
      "q3: 0.8342\n",
      "iqr: 0.0341\n",
      "count: 2340\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8338\n",
      "median: 0.834\n",
      "std: 0.0208\n",
      "min: 0.741\n",
      "max: 0.99\n",
      "q1: 0.8213\n",
      "q3: 0.8462\n",
      "iqr: 0.0249\n",
      "count: 2340\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.1346\n",
      "median: 0.1121\n",
      "std: 0.1159\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.0465\n",
      "q3: 0.197\n",
      "iqr: 0.1505\n",
      "count: 2340\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.3124\n",
      "median: 0.2895\n",
      "std: 0.2176\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.1479\n",
      "q3: 0.4545\n",
      "iqr: 0.3067\n",
      "count: 2340\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.1456\n",
      "median: 0.1429\n",
      "std: 0.094\n",
      "min: 0.0\n",
      "max: 0.5714\n",
      "q1: 0.0725\n",
      "q3: 0.2098\n",
      "iqr: 0.1373\n",
      "count: 2340\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.017\n",
      "median: 0.0044\n",
      "std: 0.0352\n",
      "min: 0.0\n",
      "max: 0.5\n",
      "q1: 0.0\n",
      "q3: 0.0225\n",
      "iqr: 0.0225\n",
      "count: 2340\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.044\n",
      "median: 0.0129\n",
      "std: 0.0795\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.0\n",
      "q3: 0.0588\n",
      "iqr: 0.0588\n",
      "count: 2340\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0186\n",
      "median: 0.0075\n",
      "std: 0.0292\n",
      "min: 0.0\n",
      "max: 0.5\n",
      "q1: 0.0\n",
      "q3: 0.029\n",
      "iqr: 0.029\n",
      "count: 2340\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.0902\n",
      "median: 0.0714\n",
      "std: 0.0861\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.0357\n",
      "q3: 0.125\n",
      "iqr: 0.0893\n",
      "count: 2340\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.2182\n",
      "median: 0.1833\n",
      "std: 0.174\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.1\n",
      "q3: 0.3\n",
      "iqr: 0.2\n",
      "count: 2340\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.0958\n",
      "median: 0.0968\n",
      "std: 0.0598\n",
      "min: 0.0\n",
      "max: 0.5714\n",
      "q1: 0.0561\n",
      "q3: 0.1312\n",
      "iqr: 0.0751\n",
      "count: 2340\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.8704\n",
      "median: 0.8962\n",
      "std: 0.0791\n",
      "min: 0.522\n",
      "max: 0.9758\n",
      "q1: 0.85\n",
      "q3: 0.9208\n",
      "iqr: 0.0708\n",
      "count: 2340\n"
     ]
    }
   ],
   "source": [
    "# run on GPT2 baseline!\n",
    "gpt_baseline_metrics = []\n",
    "with open(\"GPT_BASELINE_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        gpt_baseline_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(gpt_baseline_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.1364\n",
      "median: 0.1241\n",
      "std: 0.0922\n",
      "min: 0.0\n",
      "max: 0.8837\n",
      "q1: 0.0766\n",
      "q3: 0.178\n",
      "iqr: 0.1014\n",
      "count: 2336\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8503\n",
      "median: 0.8504\n",
      "std: 0.0262\n",
      "min: 0.7552\n",
      "max: 0.9808\n",
      "q1: 0.8345\n",
      "q3: 0.8661\n",
      "iqr: 0.0315\n",
      "count: 2336\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.83\n",
      "median: 0.8293\n",
      "std: 0.0288\n",
      "min: 0.7434\n",
      "max: 0.988\n",
      "q1: 0.812\n",
      "q3: 0.8466\n",
      "iqr: 0.0346\n",
      "count: 2336\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8397\n",
      "median: 0.8392\n",
      "std: 0.0222\n",
      "min: 0.7662\n",
      "max: 0.9803\n",
      "q1: 0.8264\n",
      "q3: 0.8523\n",
      "iqr: 0.0259\n",
      "count: 2336\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.1964\n",
      "median: 0.1766\n",
      "std: 0.1316\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.1\n",
      "q3: 0.2667\n",
      "iqr: 0.1667\n",
      "count: 2336\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.3305\n",
      "median: 0.318\n",
      "std: 0.1897\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.1818\n",
      "q3: 0.4615\n",
      "iqr: 0.2797\n",
      "count: 2336\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.2028\n",
      "median: 0.1956\n",
      "std: 0.1071\n",
      "min: 0.0\n",
      "max: 0.8602\n",
      "q1: 0.129\n",
      "q3: 0.2674\n",
      "iqr: 0.1384\n",
      "count: 2336\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.0371\n",
      "median: 0.02\n",
      "std: 0.0727\n",
      "min: 0.0\n",
      "max: 0.9167\n",
      "q1: 0.0\n",
      "q3: 0.0459\n",
      "iqr: 0.0459\n",
      "count: 2336\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.0623\n",
      "median: 0.0387\n",
      "std: 0.0862\n",
      "min: 0.0\n",
      "max: 0.8043\n",
      "q1: 0.0\n",
      "q3: 0.0833\n",
      "iqr: 0.0833\n",
      "count: 2336\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0378\n",
      "median: 0.0256\n",
      "std: 0.0649\n",
      "min: 0.0\n",
      "max: 0.8132\n",
      "q1: 0.0\n",
      "q3: 0.05\n",
      "iqr: 0.05\n",
      "count: 2336\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.1311\n",
      "median: 0.1111\n",
      "std: 0.1045\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.0658\n",
      "q3: 0.1667\n",
      "iqr: 0.1009\n",
      "count: 2336\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.2171\n",
      "median: 0.1934\n",
      "std: 0.1358\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.119\n",
      "q3: 0.2917\n",
      "iqr: 0.1726\n",
      "count: 2336\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.1323\n",
      "median: 0.1231\n",
      "std: 0.0806\n",
      "min: 0.0\n",
      "max: 0.8602\n",
      "q1: 0.0889\n",
      "q3: 0.1622\n",
      "iqr: 0.0733\n",
      "count: 2336\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.9101\n",
      "median: 0.9194\n",
      "std: 0.0448\n",
      "min: 0.5618\n",
      "max: 0.9957\n",
      "q1: 0.8945\n",
      "q3: 0.9387\n",
      "iqr: 0.0442\n",
      "count: 2336\n"
     ]
    }
   ],
   "source": [
    "# run on GPT2 finetuned!\n",
    "gpt_finetuned_metrics = []\n",
    "with open(\"GPT_FINETUNED_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        gpt_finetuned_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(gpt_finetuned_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.1044\n",
      "median: 0.0925\n",
      "std: 0.0668\n",
      "min: 0.0273\n",
      "max: 0.2616\n",
      "q1: 0.0529\n",
      "q3: 0.1274\n",
      "iqr: 0.0745\n",
      "count: 20\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8355\n",
      "median: 0.8355\n",
      "std: 0.0221\n",
      "min: 0.7928\n",
      "max: 0.8953\n",
      "q1: 0.8242\n",
      "q3: 0.8479\n",
      "iqr: 0.0237\n",
      "count: 20\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8239\n",
      "median: 0.8156\n",
      "std: 0.0316\n",
      "min: 0.781\n",
      "max: 0.8793\n",
      "q1: 0.7957\n",
      "q3: 0.8469\n",
      "iqr: 0.0512\n",
      "count: 20\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8292\n",
      "median: 0.8254\n",
      "std: 0.0193\n",
      "min: 0.803\n",
      "max: 0.8816\n",
      "q1: 0.8174\n",
      "q3: 0.8358\n",
      "iqr: 0.0184\n",
      "count: 20\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.1634\n",
      "median: 0.1474\n",
      "std: 0.11\n",
      "min: 0.0299\n",
      "max: 0.3878\n",
      "q1: 0.0753\n",
      "q3: 0.2001\n",
      "iqr: 0.1248\n",
      "count: 20\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.3176\n",
      "median: 0.2949\n",
      "std: 0.1894\n",
      "min: 0.0252\n",
      "max: 0.7561\n",
      "q1: 0.2345\n",
      "q3: 0.3636\n",
      "iqr: 0.1292\n",
      "count: 20\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.1601\n",
      "median: 0.1663\n",
      "std: 0.0825\n",
      "min: 0.0468\n",
      "max: 0.35\n",
      "q1: 0.09\n",
      "q3: 0.2051\n",
      "iqr: 0.1152\n",
      "count: 20\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.0163\n",
      "median: 0.0093\n",
      "std: 0.0221\n",
      "min: 0.0\n",
      "max: 0.0889\n",
      "q1: 0.0\n",
      "q3: 0.0178\n",
      "iqr: 0.0178\n",
      "count: 20\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.0478\n",
      "median: 0.0263\n",
      "std: 0.0507\n",
      "min: 0.0\n",
      "max: 0.1538\n",
      "q1: 0.0\n",
      "q3: 0.1\n",
      "iqr: 0.1\n",
      "count: 20\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0198\n",
      "median: 0.0136\n",
      "std: 0.0246\n",
      "min: 0.0\n",
      "max: 0.1026\n",
      "q1: 0.0\n",
      "q3: 0.0245\n",
      "iqr: 0.0245\n",
      "count: 20\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.1048\n",
      "median: 0.0801\n",
      "std: 0.0884\n",
      "min: 0.0213\n",
      "max: 0.3333\n",
      "q1: 0.0355\n",
      "q3: 0.1295\n",
      "iqr: 0.094\n",
      "count: 20\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.1871\n",
      "median: 0.1483\n",
      "std: 0.112\n",
      "min: 0.0252\n",
      "max: 0.4167\n",
      "q1: 0.1243\n",
      "q3: 0.2727\n",
      "iqr: 0.1485\n",
      "count: 20\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.0959\n",
      "median: 0.0746\n",
      "std: 0.0602\n",
      "min: 0.0381\n",
      "max: 0.275\n",
      "q1: 0.0548\n",
      "q3: 0.1222\n",
      "iqr: 0.0674\n",
      "count: 20\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.8961\n",
      "median: 0.9204\n",
      "std: 0.0672\n",
      "min: 0.7569\n",
      "max: 0.9618\n",
      "q1: 0.8613\n",
      "q3: 0.9467\n",
      "iqr: 0.0854\n",
      "count: 20\n"
     ]
    }
   ],
   "source": [
    "# run on LLAMA baseline!\n",
    "llama_baseline_metrics = []\n",
    "with open(\"LLAMA_BASELINE_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        llama_baseline_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(llama_baseline_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.2252\n",
      "median: 0.2114\n",
      "std: 0.1521\n",
      "min: 0.0766\n",
      "max: 0.8233\n",
      "q1: 0.1598\n",
      "q3: 0.2276\n",
      "iqr: 0.0678\n",
      "count: 20\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8397\n",
      "median: 0.8379\n",
      "std: 0.0234\n",
      "min: 0.8033\n",
      "max: 0.9017\n",
      "q1: 0.8283\n",
      "q3: 0.8495\n",
      "iqr: 0.0212\n",
      "count: 20\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8453\n",
      "median: 0.8414\n",
      "std: 0.0381\n",
      "min: 0.7892\n",
      "max: 0.9751\n",
      "q1: 0.8284\n",
      "q3: 0.8561\n",
      "iqr: 0.0277\n",
      "count: 20\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8421\n",
      "median: 0.8412\n",
      "std: 0.0257\n",
      "min: 0.8107\n",
      "max: 0.937\n",
      "q1: 0.8289\n",
      "q3: 0.8459\n",
      "iqr: 0.017\n",
      "count: 20\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.3494\n",
      "median: 0.3299\n",
      "std: 0.1933\n",
      "min: 0.0829\n",
      "max: 0.9348\n",
      "q1: 0.2456\n",
      "q3: 0.4103\n",
      "iqr: 0.1647\n",
      "count: 20\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.312\n",
      "median: 0.2849\n",
      "std: 0.1833\n",
      "min: 0.0549\n",
      "max: 0.7661\n",
      "q1: 0.2096\n",
      "q3: 0.3795\n",
      "iqr: 0.1699\n",
      "count: 20\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.2752\n",
      "median: 0.2655\n",
      "std: 0.1191\n",
      "min: 0.0971\n",
      "max: 0.637\n",
      "q1: 0.1979\n",
      "q3: 0.3234\n",
      "iqr: 0.1255\n",
      "count: 20\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.0935\n",
      "median: 0.0435\n",
      "std: 0.1864\n",
      "min: 0.0\n",
      "max: 0.8667\n",
      "q1: 0.0265\n",
      "q3: 0.0696\n",
      "iqr: 0.0431\n",
      "count: 20\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.0736\n",
      "median: 0.0281\n",
      "std: 0.1043\n",
      "min: 0.0\n",
      "max: 0.4432\n",
      "q1: 0.0204\n",
      "q3: 0.0734\n",
      "iqr: 0.053\n",
      "count: 20\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0679\n",
      "median: 0.0355\n",
      "std: 0.1251\n",
      "min: 0.0\n",
      "max: 0.5865\n",
      "q1: 0.0246\n",
      "q3: 0.0537\n",
      "iqr: 0.0291\n",
      "count: 20\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.2119\n",
      "median: 0.1737\n",
      "std: 0.1851\n",
      "min: 0.0552\n",
      "max: 0.8913\n",
      "q1: 0.1046\n",
      "q3: 0.2104\n",
      "iqr: 0.1058\n",
      "count: 20\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.17\n",
      "median: 0.1424\n",
      "std: 0.1073\n",
      "min: 0.0503\n",
      "max: 0.4607\n",
      "q1: 0.1108\n",
      "q3: 0.1892\n",
      "iqr: 0.0785\n",
      "count: 20\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.1575\n",
      "median: 0.1297\n",
      "std: 0.1132\n",
      "min: 0.0874\n",
      "max: 0.6074\n",
      "q1: 0.0966\n",
      "q3: 0.1648\n",
      "iqr: 0.0682\n",
      "count: 20\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.9405\n",
      "median: 0.9544\n",
      "std: 0.04\n",
      "min: 0.7965\n",
      "max: 0.9801\n",
      "q1: 0.9292\n",
      "q3: 0.962\n",
      "iqr: 0.0328\n",
      "count: 20\n"
     ]
    }
   ],
   "source": [
    "# run on LLAMA finetuned!\n",
    "llama_finetuned_metrics = []\n",
    "with open(\"LLAMA_FINETUNED_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        llama_finetuned_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(llama_finetuned_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.1612\n",
      "median: 0.1533\n",
      "std: 0.08\n",
      "min: 0.0275\n",
      "max: 0.3945\n",
      "q1: 0.103\n",
      "q3: 0.2122\n",
      "iqr: 0.1092\n",
      "count: 100\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8525\n",
      "median: 0.8536\n",
      "std: 0.0232\n",
      "min: 0.7839\n",
      "max: 0.8972\n",
      "q1: 0.8378\n",
      "q3: 0.8715\n",
      "iqr: 0.0337\n",
      "count: 100\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8407\n",
      "median: 0.838\n",
      "std: 0.0288\n",
      "min: 0.786\n",
      "max: 0.93\n",
      "q1: 0.8221\n",
      "q3: 0.86\n",
      "iqr: 0.0379\n",
      "count: 100\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8462\n",
      "median: 0.8434\n",
      "std: 0.0185\n",
      "min: 0.8005\n",
      "max: 0.8929\n",
      "q1: 0.8351\n",
      "q3: 0.8602\n",
      "iqr: 0.0251\n",
      "count: 100\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.2563\n",
      "median: 0.222\n",
      "std: 0.1558\n",
      "min: 0.0\n",
      "max: 0.8\n",
      "q1: 0.1534\n",
      "q3: 0.3333\n",
      "iqr: 0.1799\n",
      "count: 100\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.3232\n",
      "median: 0.3031\n",
      "std: 0.1828\n",
      "min: 0.0\n",
      "max: 0.8333\n",
      "q1: 0.1775\n",
      "q3: 0.4287\n",
      "iqr: 0.2512\n",
      "count: 100\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.2259\n",
      "median: 0.2313\n",
      "std: 0.0904\n",
      "min: 0.0\n",
      "max: 0.4478\n",
      "q1: 0.1681\n",
      "q3: 0.279\n",
      "iqr: 0.1109\n",
      "count: 100\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.048\n",
      "median: 0.0284\n",
      "std: 0.0675\n",
      "min: 0.0\n",
      "max: 0.5\n",
      "q1: 0.0077\n",
      "q3: 0.0672\n",
      "iqr: 0.0594\n",
      "count: 100\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.0601\n",
      "median: 0.0351\n",
      "std: 0.0736\n",
      "min: 0.0\n",
      "max: 0.4545\n",
      "q1: 0.0142\n",
      "q3: 0.0879\n",
      "iqr: 0.0738\n",
      "count: 100\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0392\n",
      "median: 0.0292\n",
      "std: 0.0368\n",
      "min: 0.0\n",
      "max: 0.1577\n",
      "q1: 0.0125\n",
      "q3: 0.0583\n",
      "iqr: 0.0459\n",
      "count: 100\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.1589\n",
      "median: 0.1302\n",
      "std: 0.1261\n",
      "min: 0.0\n",
      "max: 0.8\n",
      "q1: 0.0848\n",
      "q3: 0.2\n",
      "iqr: 0.1152\n",
      "count: 100\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.1872\n",
      "median: 0.1713\n",
      "std: 0.1102\n",
      "min: 0.0\n",
      "max: 0.6087\n",
      "q1: 0.1061\n",
      "q3: 0.2248\n",
      "iqr: 0.1187\n",
      "count: 100\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.131\n",
      "median: 0.131\n",
      "std: 0.0499\n",
      "min: 0.0\n",
      "max: 0.2553\n",
      "q1: 0.0991\n",
      "q3: 0.1584\n",
      "iqr: 0.0593\n",
      "count: 100\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.9154\n",
      "median: 0.9252\n",
      "std: 0.0364\n",
      "min: 0.8045\n",
      "max: 0.9613\n",
      "q1: 0.8969\n",
      "q3: 0.945\n",
      "iqr: 0.0481\n",
      "count: 100\n"
     ]
    }
   ],
   "source": [
    "# run on OpenAI baseline!\n",
    "openai_baseline_metrics = []\n",
    "with open(\"OPENAI_BASELINE_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        openai_baseline_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(openai_baseline_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.1855\n",
      "median: 0.1795\n",
      "std: 0.1093\n",
      "min: 0.0165\n",
      "max: 0.8582\n",
      "q1: 0.1024\n",
      "q3: 0.2546\n",
      "iqr: 0.1522\n",
      "count: 100\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.823\n",
      "median: 0.8201\n",
      "std: 0.0298\n",
      "min: 0.7669\n",
      "max: 0.9642\n",
      "q1: 0.8054\n",
      "q3: 0.8391\n",
      "iqr: 0.0336\n",
      "count: 100\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8383\n",
      "median: 0.8356\n",
      "std: 0.0279\n",
      "min: 0.7814\n",
      "max: 0.9658\n",
      "q1: 0.8184\n",
      "q3: 0.8551\n",
      "iqr: 0.0366\n",
      "count: 100\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8302\n",
      "median: 0.8301\n",
      "std: 0.023\n",
      "min: 0.7934\n",
      "max: 0.965\n",
      "q1: 0.8153\n",
      "q3: 0.8403\n",
      "iqr: 0.025\n",
      "count: 100\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.3423\n",
      "median: 0.3333\n",
      "std: 0.1898\n",
      "min: 0.0213\n",
      "max: 0.84\n",
      "q1: 0.1965\n",
      "q3: 0.4874\n",
      "iqr: 0.2909\n",
      "count: 100\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.249\n",
      "median: 0.206\n",
      "std: 0.177\n",
      "min: 0.0081\n",
      "max: 0.8889\n",
      "q1: 0.1275\n",
      "q3: 0.328\n",
      "iqr: 0.2006\n",
      "count: 100\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.214\n",
      "median: 0.2173\n",
      "std: 0.1128\n",
      "min: 0.0157\n",
      "max: 0.8571\n",
      "q1: 0.1554\n",
      "q3: 0.2622\n",
      "iqr: 0.1068\n",
      "count: 100\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.0549\n",
      "median: 0.0256\n",
      "std: 0.0998\n",
      "min: 0.0\n",
      "max: 0.8367\n",
      "q1: 0.008\n",
      "q3: 0.0583\n",
      "iqr: 0.0503\n",
      "count: 100\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.0405\n",
      "median: 0.0214\n",
      "std: 0.0939\n",
      "min: 0.0\n",
      "max: 0.8723\n",
      "q1: 0.0073\n",
      "q3: 0.0357\n",
      "iqr: 0.0284\n",
      "count: 100\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0348\n",
      "median: 0.02\n",
      "std: 0.0866\n",
      "min: 0.0\n",
      "max: 0.8542\n",
      "q1: 0.0091\n",
      "q3: 0.0389\n",
      "iqr: 0.0298\n",
      "count: 100\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.1972\n",
      "median: 0.1861\n",
      "std: 0.1331\n",
      "min: 0.0213\n",
      "max: 0.84\n",
      "q1: 0.0971\n",
      "q3: 0.254\n",
      "iqr: 0.1569\n",
      "count: 100\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.1449\n",
      "median: 0.1143\n",
      "std: 0.129\n",
      "min: 0.0081\n",
      "max: 0.875\n",
      "q1: 0.0684\n",
      "q3: 0.177\n",
      "iqr: 0.1086\n",
      "count: 100\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.1192\n",
      "median: 0.1143\n",
      "std: 0.0847\n",
      "min: 0.0157\n",
      "max: 0.8571\n",
      "q1: 0.0879\n",
      "q3: 0.1358\n",
      "iqr: 0.048\n",
      "count: 100\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.9259\n",
      "median: 0.933\n",
      "std: 0.0371\n",
      "min: 0.821\n",
      "max: 0.9934\n",
      "q1: 0.9078\n",
      "q3: 0.9521\n",
      "iqr: 0.0443\n",
      "count: 100\n"
     ]
    }
   ],
   "source": [
    "# run on OpenAI finetuned!\n",
    "openai_finetuned_metrics = []\n",
    "with open(\"OPENAI_FINETUNED_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        openai_finetuned_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(openai_finetuned_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== meteor ===\n",
      "mean: 0.1778\n",
      "median: 0.1894\n",
      "std: 0.0982\n",
      "min: 0.0\n",
      "max: 0.6995\n",
      "q1: 0.1077\n",
      "q3: 0.2345\n",
      "iqr: 0.1269\n",
      "count: 100\n",
      "\n",
      "=== bertscore_precision ===\n",
      "mean: 0.8165\n",
      "median: 0.8201\n",
      "std: 0.0335\n",
      "min: 0.6869\n",
      "max: 0.9527\n",
      "q1: 0.7983\n",
      "q3: 0.8374\n",
      "iqr: 0.0391\n",
      "count: 100\n",
      "\n",
      "=== bertscore_recall ===\n",
      "mean: 0.8359\n",
      "median: 0.8357\n",
      "std: 0.0288\n",
      "min: 0.75\n",
      "max: 0.9512\n",
      "q1: 0.8213\n",
      "q3: 0.8531\n",
      "iqr: 0.0318\n",
      "count: 100\n",
      "\n",
      "=== bertscore_f1 ===\n",
      "mean: 0.8255\n",
      "median: 0.8252\n",
      "std: 0.0236\n",
      "min: 0.7546\n",
      "max: 0.9014\n",
      "q1: 0.8137\n",
      "q3: 0.8372\n",
      "iqr: 0.0235\n",
      "count: 100\n",
      "\n",
      "=== rouge1_precision ===\n",
      "mean: 0.3336\n",
      "median: 0.3012\n",
      "std: 0.2018\n",
      "min: 0.0\n",
      "max: 0.9474\n",
      "q1: 0.1756\n",
      "q3: 0.4743\n",
      "iqr: 0.2987\n",
      "count: 100\n",
      "\n",
      "=== rouge1_recall ===\n",
      "mean: 0.2368\n",
      "median: 0.2157\n",
      "std: 0.1615\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.1121\n",
      "q3: 0.3412\n",
      "iqr: 0.2291\n",
      "count: 100\n",
      "\n",
      "=== rouge1_f1 ===\n",
      "mean: 0.206\n",
      "median: 0.1997\n",
      "std: 0.1047\n",
      "min: 0.0\n",
      "max: 0.4284\n",
      "q1: 0.1283\n",
      "q3: 0.284\n",
      "iqr: 0.1558\n",
      "count: 100\n",
      "\n",
      "=== rouge2_precision ===\n",
      "mean: 0.0601\n",
      "median: 0.032\n",
      "std: 0.1064\n",
      "min: 0.0\n",
      "max: 0.8571\n",
      "q1: 0.0052\n",
      "q3: 0.0775\n",
      "iqr: 0.0723\n",
      "count: 100\n",
      "\n",
      "=== rouge2_recall ===\n",
      "mean: 0.031\n",
      "median: 0.0177\n",
      "std: 0.037\n",
      "min: 0.0\n",
      "max: 0.2286\n",
      "q1: 0.0059\n",
      "q3: 0.0464\n",
      "iqr: 0.0405\n",
      "count: 100\n",
      "\n",
      "=== rouge2_f1 ===\n",
      "mean: 0.0306\n",
      "median: 0.0194\n",
      "std: 0.0429\n",
      "min: 0.0\n",
      "max: 0.3609\n",
      "q1: 0.0061\n",
      "q3: 0.0446\n",
      "iqr: 0.0386\n",
      "count: 100\n",
      "\n",
      "=== rougeL_precision ===\n",
      "mean: 0.1976\n",
      "median: 0.1635\n",
      "std: 0.1579\n",
      "min: 0.0\n",
      "max: 0.9123\n",
      "q1: 0.0851\n",
      "q3: 0.239\n",
      "iqr: 0.1539\n",
      "count: 100\n",
      "\n",
      "=== rougeL_recall ===\n",
      "mean: 0.135\n",
      "median: 0.1132\n",
      "std: 0.1183\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "q1: 0.0615\n",
      "q3: 0.1775\n",
      "iqr: 0.116\n",
      "count: 100\n",
      "\n",
      "=== rougeL_f1 ===\n",
      "mean: 0.1118\n",
      "median: 0.1056\n",
      "std: 0.0549\n",
      "min: 0.0\n",
      "max: 0.3881\n",
      "q1: 0.0747\n",
      "q3: 0.145\n",
      "iqr: 0.0702\n",
      "count: 100\n",
      "\n",
      "=== cosine_similarity ===\n",
      "mean: 0.9164\n",
      "median: 0.931\n",
      "std: 0.0547\n",
      "min: 0.6731\n",
      "max: 0.9792\n",
      "q1: 0.8971\n",
      "q3: 0.9537\n",
      "iqr: 0.0566\n",
      "count: 100\n"
     ]
    }
   ],
   "source": [
    "# run on Ensemble!\n",
    "ensemble_metrics = []\n",
    "with open(\"ENSEMBLE_METRICS.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        ensemble_metrics.append(ast.literal_eval(line.strip()))\n",
    "print_analysis(analyze_metrics(ensemble_metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
