{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Exploration\n",
    "\n",
    "This notebook illustrates the setup of a BERT model used for determining similar responses to a patients query based on the the data it has previously seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# gather all imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import  AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading in all the data previously processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(file_path):\n",
    "    \"\"\"\n",
    "    Helper function to load in the data into a specific form \n",
    "\n",
    "    @PARAMS:\n",
    "        - file_path -> the file to process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # format data to be just question answer pairs\n",
    "        formatted_data = []\n",
    "        for entry in data:\n",
    "            formatted_data.append({\n",
    "                \"question\": entry[\"Question\"],\n",
    "                \"response\": entry[\"Answer\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"Loaded {len(formatted_data)} Q&A pairs from {file_path}!\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading in file...\\n{e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18749 Q&A pairs from processed_data/train.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/validation.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/test.json!\n",
      "{'question': 'will eating late evening meals increase my cholesterol?', 'response': 'no. it is what you are eating (as well as your genetics) not when you eat it. it depends on the kinds of foods that you eat. make sure that you are eating healthy foods in order to not gain great amount of cholesterol. you have to always watch what you eat in order to have a healthy skin and body. you may check out www. clearclinic. com for great ideas to achieve an acne free skin.'}\n",
      "{'question': 'who is affected by arthritis?', 'response': 'arthritis sufferers include men and women children and adults. approximately 350 million people worldwide have arthritis. nearly 40 million people in the united states are affected by arthritis including over a quarter million children! more than 27 million americans have osteoarthritis. approximately 1. 3 million americans suffer from rheumatoid arthritis. more than half of those with arthritis are under 65 years of age. nearly 60% of americans with arthritis are women.'}\n",
      "{'question': 'can i be pregnant if i had unprotected sex the 4th day of being on the depo?', 'response': 'yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.'}\n"
     ]
    }
   ],
   "source": [
    "# load in formatted data\n",
    "## TRAIN ##\n",
    "train_data = load_and_prepare(\"processed_data/train.json\")\n",
    "\n",
    "## VAL ##\n",
    "val_data = load_and_prepare(\"processed_data/validation.json\")\n",
    "\n",
    "## TEST ##\n",
    "test_data = load_and_prepare(\"processed_data/test.json\")\n",
    "\n",
    "# print out one value of each to make sure it is loaded correctly\n",
    "print(train_data[0])\n",
    "print(val_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "For the model, I will use a BERT pre-trained model to build a set of recomendations based on a confidence score. Meaning that I will not fine-tune the model based on the patient doctor interactions. The goal is to build a text generation model and pass in the recomendations from here as an input. This is a hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT:\n",
    "    \"\"\"\n",
    "    This class is the implementation of a BERT recommendation model.\n",
    "    Without tuning, this model will recomend a response based on a set of confidence values that come from the set of all interactions.\n",
    "\n",
    "    This will be passed to a generative model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", use_gpu=True):\n",
    "        \"\"\"\n",
    "        Initializer function to establish the pre-trained model\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_name -> specifc pre-trained model to initialize\n",
    "            - use_gpu    -> speed up runtime by using GPU if available\n",
    "        \"\"\"\n",
    "        # empty cache if possible\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # values for the response\n",
    "        self.data = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def get_embeddings(self, questions, batch_size=16):\n",
    "        \"\"\"\n",
    "        Function to return a set of question embeddings.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - questions  -> the patient query\n",
    "            - batch_size -> batch split to save time\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # process in batches to save time\n",
    "            for i in range(0, len(questions), batch_size):\n",
    "                batch_questions = questions[i:i + batch_size]\n",
    "                \n",
    "                # tokenize this batch\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_questions,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # get the BERT outputs\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "                # convert to numpy\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                # print progress!!\n",
    "                if (i + 1) % (batch_size * 10) == 0:\n",
    "                    print(f\"Processed {i + 1}/{len(questions)} questions\")\n",
    "                \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def embed_data(self, data, batch_size=16):\n",
    "        \"\"\"\n",
    "        Function to convert the data into a form that includes the embeddings.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - data       -> patient and doctor interactions\n",
    "            - batch_size -> batch split to save time\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "        questions = [point['question'] for point in data]\n",
    "        #responses = [point['response'] for point in data]\n",
    "\n",
    "        # now get the embeddings for the q/r\n",
    "        embeddings = self.get_embeddings(questions, batch_size)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # finally return all info with the embeddings\n",
    "        return Dataset.from_dict({\n",
    "            'question': questions,\n",
    "            'embedding': embeddings.tolist()\n",
    "        })\n",
    "    \n",
    "    def get_similar_responses(self, question, amount=3, threshold=.7):\n",
    "        \"\"\" \n",
    "        Function to search through all the data and get the similar interaction cases.\n",
    "\n",
    "        @PARAMS:\n",
    "            - question  -> the input to search against\n",
    "            - amount    -> how many to return\n",
    "            - threshold -> cosine similarity results must return above this\n",
    "        \"\"\"\n",
    "        if self.data is None or self.embeddings is None:\n",
    "            raise ValueError(\"No data accessible...\")\n",
    "        \n",
    "        # start by getting the embedding for the question input\n",
    "        question_embedding = self.get_embeddings([question])\n",
    "\n",
    "        # now run cosine similarity against the previously calculated data embeddings\n",
    "        results = cosine_similarity(question_embedding, self.embeddings)[0]\n",
    "        # based on the amount input, get those top values\n",
    "        top_indices = np.argsort(results)[-amount:][::-1]\n",
    "\n",
    "        # now loop through the top values and append only the ones that meet the threshold\n",
    "        similar = []\n",
    "        for index in top_indices:\n",
    "            if results[index] >= threshold:\n",
    "                similar.append({\n",
    "                    'question': self.data[index]['question'],\n",
    "                    'response': self.data[index]['response'],\n",
    "                    'similarity_score': float(results[index]),\n",
    "                })\n",
    "        return similar\n",
    "    \n",
    "    def get_recommendation(self, question):\n",
    "        \"\"\" \n",
    "        Function to return a recommendation of a response based on a user input.\n",
    "\n",
    "        @PARAMS:\n",
    "            - question -> the input from the patient waiting for the returned doctor response\n",
    "        \"\"\"\n",
    "        # first get all similar cases\n",
    "        similar_cases = self.get_similar_responses(question)\n",
    "\n",
    "        if similar_cases:\n",
    "            return {\n",
    "                \"similarity_score\": similar_cases[0]['similarity_score'],\n",
    "                \"most_similar_question\": similar_cases[0]['question'],\n",
    "                \"recommendation\": similar_cases[0]['response'],\n",
    "                \"other_matches\": [\n",
    "                    {\n",
    "                        'similarity_score': case['similarity_score'], \n",
    "                        'question': case['question'],\n",
    "                        \"response\": case['response']\n",
    "                    } for case in similar_cases[1:]\n",
    "                ]\n",
    "            }\n",
    "        # o/w return a default answer suggesting for seeking other advice\n",
    "        else:\n",
    "            return {\n",
    "                \"similarity_score\": 0.0,\n",
    "                \"recommendation\": \"I recommend consulting a healthcare provider for a proper evaluation.\",\n",
    "                \"other_matches\": []\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the model to see how it does against test inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# build the baseline model!\n",
    "baseline_bert = BERT()\n",
    "baseline_bert.embed_data(train_data)\n",
    "\n",
    "# now loop through all the testing examples and write to a file the specific outputs!    \n",
    "with open(\"BERT_BASELINE_results.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(\"BERT MODEL EVALUATION RESULTS\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_data, 1):\n",
    "        f.write(f\"Test Case {i}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Write the test question\n",
    "        f.write(\"Patient Question:\\n\")\n",
    "        f.write(test_case['question'])\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Write the actual doctor's response\n",
    "        f.write(\"Actual Doctor's Response:\\n\")\n",
    "        f.write(test_case['response'])\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Get model's recommendation\n",
    "        result = baseline_bert.get_recommendation(test_case['question'])\n",
    "        \n",
    "        # Write model's response\n",
    "        f.write(\"Model's Recommendation:\\n\")\n",
    "        f.write(f\"Similarity Score: {result['similarity_score']:.4f}\\n\")\n",
    "        f.write(result['recommendation'])\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Write separator between test cases\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
