{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble\n",
    "\n",
    "This is the notebook detailing the model ensemble of passing in the BERT output as context for a user query. The idea is simple, based on a testing example, the BERT model will be called first and all output will be fed into another model for then interpretation to see if the results improve. For this testing, I will be using the fine-tuned version of GPT-3.5-Turbo due to its speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import  AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, start by loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(file_path):\n",
    "    \"\"\"\n",
    "    Helper function to load in the data into a specific form \n",
    "\n",
    "    @PARAMS:\n",
    "        - file_path -> the file to process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # format data to be just question answer pairs\n",
    "        formatted_data = []\n",
    "        for entry in data:\n",
    "            formatted_data.append({\n",
    "                \"question\": entry[\"Question\"],\n",
    "                \"response\": entry[\"Answer\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"Loaded {len(formatted_data)} Q&A pairs from {file_path}!\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading in file...\\n{e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18749 Q&A pairs from processed_data/train.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/validation.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/test.json!\n",
      "{'question': 'will eating late evening meals increase my cholesterol?', 'response': 'no. it is what you are eating (as well as your genetics) not when you eat it. it depends on the kinds of foods that you eat. make sure that you are eating healthy foods in order to not gain great amount of cholesterol. you have to always watch what you eat in order to have a healthy skin and body. you may check out www. clearclinic. com for great ideas to achieve an acne free skin.'}\n",
      "{'question': 'who is affected by arthritis?', 'response': 'arthritis sufferers include men and women children and adults. approximately 350 million people worldwide have arthritis. nearly 40 million people in the united states are affected by arthritis including over a quarter million children! more than 27 million americans have osteoarthritis. approximately 1. 3 million americans suffer from rheumatoid arthritis. more than half of those with arthritis are under 65 years of age. nearly 60% of americans with arthritis are women.'}\n",
      "{'question': 'can i be pregnant if i had unprotected sex the 4th day of being on the depo?', 'response': 'yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.'}\n"
     ]
    }
   ],
   "source": [
    "# load in formatted data\n",
    "## TRAIN ##\n",
    "train_data = load_and_prepare(\"processed_data/train.json\")\n",
    "\n",
    "## VAL ##\n",
    "val_data = load_and_prepare(\"processed_data/validation.json\")\n",
    "\n",
    "## TEST ##\n",
    "test_data = load_and_prepare(\"processed_data/test.json\")\n",
    "\n",
    "# print out one value of each to make sure it is loaded correctly\n",
    "print(train_data[0])\n",
    "print(val_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coppying the exact BERT model from BERT.ipynb and using this for input to the text generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT:\n",
    "    \"\"\"\n",
    "    This class is the implementation of a BERT recommendation model.\n",
    "    Without tuning, this model will recomend a response based on a set of confidence values that come from the set of all interactions.\n",
    "\n",
    "    This will be passed to a generative model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", use_gpu=True):\n",
    "        \"\"\"\n",
    "        Initializer function to establish the pre-trained model\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_name -> specifc pre-trained model to initialize\n",
    "            - use_gpu    -> speed up runtime by using GPU if available\n",
    "        \"\"\"\n",
    "        # empty cache if possible\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # values for the response\n",
    "        self.data = None\n",
    "        self.embeddings = None\n",
    "\n",
    "    def get_embeddings(self, questions, batch_size=16):\n",
    "        \"\"\"\n",
    "        Function to return a set of question embeddings.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - questions  -> the patient query\n",
    "            - batch_size -> batch split to save time\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # process in batches to save time\n",
    "            for i in range(0, len(questions), batch_size):\n",
    "                batch_questions = questions[i:i + batch_size]\n",
    "                \n",
    "                # tokenize this batch\n",
    "                inputs = self.tokenizer(\n",
    "                    batch_questions,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # get the BERT outputs\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "                # convert to numpy\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                # print progress!!\n",
    "                if (i + 1) % (batch_size * 10) == 0:\n",
    "                    print(f\"Processed {i + 1}/{len(questions)} questions\")\n",
    "                \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def embed_data(self, data, batch_size=16):\n",
    "        \"\"\"\n",
    "        Function to convert the data into a form that includes the embeddings.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - data       -> patient and doctor interactions\n",
    "            - batch_size -> batch split to save time\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "        questions = [point['question'] for point in data]\n",
    "        #responses = [point['response'] for point in data]\n",
    "\n",
    "        # now get the embeddings for the q/r\n",
    "        embeddings = self.get_embeddings(questions, batch_size)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # finally return all info with the embeddings\n",
    "        return Dataset.from_dict({\n",
    "            'question': questions,\n",
    "            'embedding': embeddings.tolist()\n",
    "        })\n",
    "    \n",
    "    def get_similar_responses(self, question, amount=3, threshold=.7):\n",
    "        \"\"\" \n",
    "        Function to search through all the data and get the similar interaction cases.\n",
    "\n",
    "        @PARAMS:\n",
    "            - question  -> the input to search against\n",
    "            - amount    -> how many to return\n",
    "            - threshold -> cosine similarity results must return above this\n",
    "        \"\"\"\n",
    "        if self.data is None or self.embeddings is None:\n",
    "            raise ValueError(\"No data accessible...\")\n",
    "        \n",
    "        # start by getting the embedding for the question input\n",
    "        question_embedding = self.get_embeddings([question])\n",
    "\n",
    "        # now run cosine similarity against the previously calculated data embeddings\n",
    "        results = cosine_similarity(question_embedding, self.embeddings)[0]\n",
    "        # based on the amount input, get those top values\n",
    "        top_indices = np.argsort(results)[-amount:][::-1]\n",
    "\n",
    "        # now loop through the top values and append only the ones that meet the threshold\n",
    "        similar = []\n",
    "        for index in top_indices:\n",
    "            if results[index] >= threshold:\n",
    "                similar.append({\n",
    "                    'question': self.data[index]['question'],\n",
    "                    'response': self.data[index]['response'],\n",
    "                    'similarity_score': float(results[index]),\n",
    "                })\n",
    "        return similar\n",
    "    \n",
    "    def get_recommendation(self, question):\n",
    "        \"\"\" \n",
    "        Function to return a recommendation of a response based on a user input.\n",
    "\n",
    "        @PARAMS:\n",
    "            - question -> the input from the patient waiting for the returned doctor response\n",
    "        \"\"\"\n",
    "        # first get all similar cases\n",
    "        similar_cases = self.get_similar_responses(question)\n",
    "\n",
    "        if similar_cases:\n",
    "            return {\n",
    "                \"similarity_score\": similar_cases[0]['similarity_score'],\n",
    "                \"most_similar_question\": similar_cases[0]['question'],\n",
    "                \"recommendation\": similar_cases[0]['response'],\n",
    "                \"other_matches\": [\n",
    "                    {\n",
    "                        'similarity_score': case['similarity_score'], \n",
    "                        'question': case['question'],\n",
    "                        \"response\": case['response']\n",
    "                    } for case in similar_cases[1:]\n",
    "                ]\n",
    "            }\n",
    "        # o/w return a default answer suggesting for seeking other advice\n",
    "        else:\n",
    "            return {\n",
    "                \"similarity_score\": 0.0,\n",
    "                \"recommendation\": \"I recommend consulting a healthcare provider for a proper evaluation.\",\n",
    "                \"other_matches\": []\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the OpenAI code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the .env file\n",
    "with open(\"../.env\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# extract the key from the first line\n",
    "line = content[0].strip()\n",
    "\n",
    "# pattern to capture key after '='\n",
    "pattern = r'API_KEY\\s*=\\s*(\\S+)'\n",
    "\n",
    "# assign key\n",
    "match = re.findall(pattern, line)\n",
    "\n",
    "if match:\n",
    "    API_KEY = match[0]\n",
    "else:\n",
    "    print(\"API_KEY not found!\")\n",
    "\n",
    "# first connect with the API_KEY we just generated\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ID info refer back to openai.ipynb\n",
    "fine_tuned_model = client.fine_tuning.jobs.retrieve('ftjob-q2v0M5wDwhtPyEYGaMdJk4BI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run the model and save the contents to a file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the BERT model!\n",
    "bert = BERT()\n",
    "bert.embed_data(train_data)\n",
    "\n",
    "# get test questions - run on 100 to save money\n",
    "questions = [point['question'] for point in test_data[:100]]\n",
    "responses = [point['response'] for point in test_data[:100]]\n",
    "\n",
    "with open('ensemble_results.txt', \"w\", encoding='utf-8') as f:\n",
    "    for question, response in zip(questions, responses):\n",
    "        # first get bert response\n",
    "        bert_response = bert.get_recommendation(question)\n",
    "\n",
    "        # craft context for fine-tuned model\n",
    "        system_message = \"You are a medical expert helping patients talk through their concerns.\"\n",
    "        user_message = f\"\"\"\n",
    "                Attached is some context and a question, using the context come up with an accurate answer to the user's query. \n",
    "                The context is some real doctor responses to questions that are similar, if relevant, use these real responses to craft an answer to the user query. \n",
    "                Do not just copy the contents of the attached context as it might be answering a different question, just use it as support when crafting your answer.\n",
    "                Only return the answer and make sure the information is accurate.\n",
    "                \n",
    "                Context: {bert_response}\n",
    "                Question: {question}\n",
    "\n",
    "        \"\"\"\n",
    "        # now get fine-tuned response\n",
    "        fine_tune_completion = client.chat.completions.create(\n",
    "            model=\"ft:gpt-3.5-turbo-0125:personal::AWJTxERx\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Answer: {fine_tune_completion.choices[0].message.content}\\n\")\n",
    "        f.write(f\"Expected Response: {response}\\n\")\n",
    "        # response seperator\n",
    "        f.write(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
