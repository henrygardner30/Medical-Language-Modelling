{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doctor Response Generation GPT-2\n",
    "\n",
    "This file explores the main goal of the project, generating a text response from a patient query.\n",
    "\n",
    "Model: https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all imports\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast, GPT2LMHeadModel, AdamW, get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import os\n",
    "import platform\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, lets load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare(file_path):\n",
    "    \"\"\"\n",
    "    Helper function to load in the data into a specific form \n",
    "\n",
    "    @PARAMS:\n",
    "        - file_path -> the file to process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # format data to be just question answer pairs\n",
    "        formatted_data = []\n",
    "        for entry in data:\n",
    "            formatted_data.append({\n",
    "                \"question\": entry[\"Question\"],\n",
    "                \"response\": entry[\"Answer\"]\n",
    "            })\n",
    "        \n",
    "        print(f\"Loaded {len(formatted_data)} Q&A pairs from {file_path}!\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading in file...\\n{e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18749 Q&A pairs from processed_data/train.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/validation.json!\n",
      "Loaded 2344 Q&A pairs from processed_data/test.json!\n",
      "{'question': 'will eating late evening meals increase my cholesterol?', 'response': 'no. it is what you are eating (as well as your genetics) not when you eat it. it depends on the kinds of foods that you eat. make sure that you are eating healthy foods in order to not gain great amount of cholesterol. you have to always watch what you eat in order to have a healthy skin and body. you may check out www. clearclinic. com for great ideas to achieve an acne free skin.'}\n",
      "{'question': 'who is affected by arthritis?', 'response': 'arthritis sufferers include men and women children and adults. approximately 350 million people worldwide have arthritis. nearly 40 million people in the united states are affected by arthritis including over a quarter million children! more than 27 million americans have osteoarthritis. approximately 1. 3 million americans suffer from rheumatoid arthritis. more than half of those with arthritis are under 65 years of age. nearly 60% of americans with arthritis are women.'}\n",
      "{'question': 'can i be pregnant if i had unprotected sex the 4th day of being on the depo?', 'response': 'yes you can. the depo will take about a month or two to take full effect. even then it is not 100% effective.'}\n"
     ]
    }
   ],
   "source": [
    "# load in formatted data\n",
    "## TRAIN ##\n",
    "train_data = load_and_prepare(\"processed_data/train.json\")\n",
    "\n",
    "## VAL ##\n",
    "val_data = load_and_prepare(\"processed_data/validation.json\")\n",
    "\n",
    "## TEST ##\n",
    "test_data = load_and_prepare(\"processed_data/test.json\")\n",
    "\n",
    "# print out one value of each to make sure it is loaded correctly\n",
    "print(train_data[0])\n",
    "print(val_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the GPT text gen model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBaseline:\n",
    "    \"\"\"\n",
    "    Simple baseline GPT-2 model without fine-tuning.\n",
    "    Used to see how well it can respond to medical queries \"out of the box\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initializer function to establish the gpt model\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_name -> which model to initialize\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # load model and tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # set pad token to eos token (needed for GPT-2)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "    def generate_response(self, question, max_length=200):\n",
    "        \"\"\"\n",
    "        Generates a response off the baseline gpt model.\n",
    "\n",
    "        @PARAMS:\n",
    "            - question   -> user query\n",
    "            - max_length -> response maximum length\n",
    "        \"\"\"\n",
    "        # get prompt formatting\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        \n",
    "        # tokenize the input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # get the output with specific parameters\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # decode the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # clean the response and return it\n",
    "        try:\n",
    "            response = response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "            # remove any continuation\n",
    "            if \"Question:\" in response:\n",
    "                response = response.split(\"Question:\")[0].strip()\n",
    "            if \"Q:\" in response:\n",
    "                response = response.split(\"Q:\")[0].strip()\n",
    "        except:\n",
    "            response = \"Error generating response.\"\n",
    "        # return response\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the model on a few inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Medical Responses:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "baseline = GPTBaseline()\n",
    "\n",
    "# lets get 5 example questions to run on from the test set\n",
    "test_questions = [point['question'] for point in test_data]\n",
    "test_responses = [point['response'] for point in test_data]\n",
    "\n",
    "print(\"Generating Medical Responses:\\n\")\n",
    "with open('gpt2_baseline_results.txt', \"w\", encoding='utf-8') as f:\n",
    "    for question,response in zip(test_questions, test_responses):\n",
    "        model_response = baseline.generate_response(question)\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Baseline Response: {model_response}\\n\")\n",
    "        f.write(f\"\\nExpected Response: {response}\\n\")\n",
    "        # response seperator\n",
    "        f.write(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay sweet we are getting results! However, they are not what we are looking for - at all. In fact, reading through some of these are pretty funny as they are just nonsense. For example when asked\n",
    "\n",
    "*who manufactures actos? my mother canâ€™t afford it & sometimes skips her dose. do they offer an assistance program?*\n",
    "\n",
    "the first sentence in the response is: there are many, many products available that will help you find the right product.\n",
    "\n",
    "Clearly there is some work to do. The goal now is to fine-tune off the training data to get more doctor-like responses. The goal isn't to replace a doctor, rather develop a model that can speak like one and hopefully give medically accurate advice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT-2\n",
    "\n",
    "Now we will fine-tune the model based on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# make sure we have CUDA enabled!\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aid from https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "class GPTFineTuned:\n",
    "    \"\"\"Class to define the Fine-Tuned version of GPT-2.\"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        Initializer function to make a GPT-2 model that will be fine-tuned.\n",
    "\n",
    "        @PARAMS:\n",
    "            - model_name -> which model to start with\n",
    "        \"\"\"\n",
    "        # make sure to run on CUDA since this will be computationally expensive\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # load tokenizer and model\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # set the pad token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # give a summary of the model size \n",
    "        model_size = sum(p.numel() for p in self.model.parameters()) / 1000000\n",
    "        print(f\"Model size: {model_size:.2f}M parameters\")\n",
    "\n",
    "    def prepare_data(self, conversations, batch_size, is_training=True):\n",
    "        \"\"\"\n",
    "        Function to read in the conversation data and convert it to a question/answer format.\n",
    "\n",
    "        @PARAMS:\n",
    "            - conversations -> the patient doctor conversation\n",
    "            - batch_size    -> parameter for the dataloader \n",
    "            - is_training   -> parameter used for shuffling the data to preserve randomness\n",
    "        \"\"\"\n",
    "        # re-format the conversations\n",
    "        texts = [f\"Question: {conv['question']}\\nAnswer: {conv['response']}\" \n",
    "                for conv in conversations]\n",
    "        \n",
    "        # tokenize the texts\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # necessary formatting\n",
    "        input_ids = encodings['input_ids']\n",
    "        attention_masks = encodings['attention_mask']\n",
    "        \n",
    "        # create the dataset in a dictionary format\n",
    "        dataset_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_masks,\n",
    "            'labels': input_ids.clone()\n",
    "        }\n",
    "        \n",
    "        # load the dictionary data into the correct format\n",
    "        data_loader = DataLoader(\n",
    "            TensorDictDataset(dataset_dict),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=is_training\n",
    "        )\n",
    "        \n",
    "        return data_loader\n",
    "    \n",
    "    def train_model(self, num_epochs=3, learning_rate=5e-5, warmup_ratio=0.1, save_path=None, gradient_accumulation_steps=2, batch_size=8):\n",
    "        \"\"\" \n",
    "        MAIN TRAINING FUNCTION.\n",
    "        \n",
    "        @PARAMS:\n",
    "            - [num_epochs, learning_rate, warmup_ratio, save_path, gradient_accumulation_steps, batch_size] -> hyperparameters to tune for better training results\n",
    "        \"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "        print(f\"Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "        \n",
    "        # aid from https://pytorch.org/docs/stable/notes/amp_examples.html use gradiant scaler\n",
    "        # initialize gradient scaler for faster training while preserving accuracy\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # optimizer with weight decay \n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # scheduler to adjust learning rate\n",
    "        num_training_steps = len(self.train_loader) * num_epochs\n",
    "        num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "        scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # values to store intermediate steps\n",
    "        best_val_loss = float('inf')\n",
    "        total_steps = 0\n",
    "        \n",
    "        # now run each epoch - one full pass through the data each!\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # reset values and start training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # update terminal output\n",
    "            progress_bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "            \n",
    "            # run the batch on the GPU! I attempted this on CPU and took forever (as expected) so dramatically speed up with batch processing!\n",
    "            for batch_idx, batch in progress_bar:\n",
    "                # move the batch to the GPU - this will dramatically save time!\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # forward pass on GPU\n",
    "                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = outputs.loss / gradient_accumulation_steps\n",
    "                \n",
    "                # backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                train_loss += loss.item() * gradient_accumulation_steps\n",
    "                \n",
    "                # update the weights, clip gradients, update learning rate!\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    total_steps += 1\n",
    "                \n",
    "                # update terminal output\n",
    "                progress_bar.set_description(f\"Loss: {train_loss/(batch_idx+1):.4f}\")\n",
    "                \n",
    "                # save a checkpoint every 1000 steps in case of crashes\n",
    "                if total_steps > 0 and total_steps % 1000 == 0:\n",
    "                    self.model.save_pretrained(f\"{save_path}_step_{total_steps}\")\n",
    "                    print(f\"\\nCheckpoint saved at step {total_steps}\")\n",
    "            \n",
    "            # update the training loss\n",
    "            avg_train_loss = train_loss / len(self.train_loader)\n",
    "            \n",
    "            # now we need to evaluate the model!\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            # now run the validation data through the model\n",
    "            print(\"\\nRunning validation...\")\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(self.val_loader):\n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                    outputs = self.model(**batch)\n",
    "                    val_loss += outputs.loss.item()\n",
    "            \n",
    "            # get the valdidation loss\n",
    "            avg_val_loss = val_loss / len(self.val_loader)\n",
    "            \n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # save the model!!! but we really only care if it performed better than the previous, so only save if it is the best one seen (so far)\n",
    "            if save_path and avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.model.save_pretrained(save_path)\n",
    "                self.tokenizer.save_pretrained(save_path)\n",
    "                print(f\"Best model saved to {save_path}\")\n",
    "\n",
    "    def generate_response(self, text, max_length=200):\n",
    "        \"\"\"\n",
    "        Function to return model output based on a user query!\n",
    "\n",
    "        @PARAMS:\n",
    "            - text       -> query to run off of\n",
    "            - max_length -> response limit\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                # we don't want the model to be too creative as this is usually info that needs to be accurate, therefore make the temperature medium level for expected tokens more likely\n",
    "                temperature=0.5,\n",
    "                top_p=0.9,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "class TensorDictDataset(Dataset):\n",
    "    \"\"\"REQUIRED CLASS for pytorch's DataLoader, which requires both a __getitem__ and __len__ function.\"\"\"\n",
    "    def __init__(self, tensor_dict):\n",
    "        self.tensor_dict = tensor_dict\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tensor_dict.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.tensor_dict.values())))\n",
    "\n",
    "class MedicalGPT(GPTFineTuned):\n",
    "    def prepare_separate_datasets(self, train_data, val_data, batch_size=8):\n",
    "        \"\"\"\n",
    "        Prepare separate training and validation datasets\n",
    "        \"\"\"\n",
    "        print(f\"Processing {len(train_data)} training examples...\")\n",
    "        self.train_loader = self.prepare_data(\n",
    "            train_data, \n",
    "            batch_size=batch_size,\n",
    "            is_training=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Processing {len(val_data)} validation examples...\")\n",
    "        self.val_loader = self.prepare_data(\n",
    "            val_data, \n",
    "            batch_size=batch_size,\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        print(\"Data preparation complete!\")\n",
    "        return self.train_loader, self.val_loader\n",
    "    \n",
    "    def generate_medical_response(self, question, max_length=200):\n",
    "        \"\"\"\n",
    "        Generate a medical response with improved parameters for better quality\n",
    "        \n",
    "        @PARAMS:\n",
    "            - question   -> user query\n",
    "            - max_length -> maximum length of response\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        prompt = f\"Question: {question}\\nAnswer:\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate with tuned parameters\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                # focus the responses - make the next token be one that is more likely. We don't want the model to be too creative with this task.\n",
    "                temperature=0.3,\n",
    "                top_p=0.85,\n",
    "                top_k=50,\n",
    "                no_repeat_ngram_size=4,\n",
    "                min_length=50,\n",
    "                repetition_penalty=1.2,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and clean the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Improved response cleaning\n",
    "        try:\n",
    "            # Remove the original question and prompt\n",
    "            response = response.split(\"Answer:\")[-1].strip()\n",
    "            \n",
    "            # Remove any repeated questions\n",
    "            if \"Question:\" in response:\n",
    "                response = response.split(\"Question:\")[0].strip()\n",
    "            \n",
    "            # Remove any Q: format questions\n",
    "            if \"Q:\" in response:\n",
    "                response = response.split(\"Q:\")[0].strip()\n",
    "            \n",
    "            # Clean up repetitive phrases\n",
    "            response = self.clean_repetitive_text(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            response = \"I apologize, but I couldn't generate a proper medical response. Please consult a healthcare professional for medical advice.\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def clean_repetitive_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean up repetitive phrases and improve formatting\n",
    "        \"\"\"\n",
    "        # Split into sentences\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Remove duplicate sentences\n",
    "        seen_sentences = set()\n",
    "        cleaned_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip().lower()\n",
    "            if sentence and sentence not in seen_sentences:\n",
    "                seen_sentences.add(sentence)\n",
    "                cleaned_sentences.append(sentence)\n",
    "        \n",
    "        # Capitalize first letters and rejoin\n",
    "        cleaned_text = '. '.join(s.capitalize() for s in cleaned_sentences if s)\n",
    "        \n",
    "        # Add final period if missing\n",
    "        if cleaned_text and not cleaned_text.endswith('.'):\n",
    "            cleaned_text += '.'\n",
    "            \n",
    "        return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep prevention activated for Windows\n",
      "Model size: 124.44M parameters\n",
      "Processing 18749 training examples...\n",
      "Processing 2344 validation examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete!\n",
      "Starting training...\n",
      "Batch size: 4\n",
      "Gradient accumulation steps: 4\n",
      "Effective batch size: 16\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1611:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4000/4688 [06:17<02:13,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1612:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4001/4688 [06:17<03:20,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1611:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4002/4688 [06:18<04:24,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1611:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4004/4688 [06:19<04:21,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.1131: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4688/4688 [07:23<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:30<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.1131\n",
      "Average validation loss: 1.7759\n",
      "Best model saved to ./medical_gpt2_model\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7930:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3311/4688 [05:11<02:06, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7927:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3313/4688 [05:12<06:44,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7929:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3314/4688 [05:13<08:41,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7928:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3316/4688 [05:14<08:38,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7893: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4688/4688 [07:22<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:30<00:00, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.7893\n",
      "Average validation loss: 1.7106\n",
      "Best model saved to ./medical_gpt2_model\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7216:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2624/4688 [04:06<06:45,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7216:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2625/4688 [04:07<10:01,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7216:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2626/4688 [04:08<13:06,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7213:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2628/4688 [04:09<12:52,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7086: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4688/4688 [07:22<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:30<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 1.7086\n",
      "Average validation loss: 1.6788\n",
      "Best model saved to ./medical_gpt2_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can reduce your blood pressure by: taking a blood pressure medication such as prednisone. using a blood thinning medication such a metronidazole. using an over-the-counter medication such aspirin. taking a medication that contains caffeine. taking an antihistamine such as cetirizine. taking medications that contain a substance called a diuretic. taking antihistamines such as naproxen. taking anticonvulsants such as metroniramate. taking metronitrate. taking diuretics such as ibuprofen.\n"
     ]
    }
   ],
   "source": [
    "# prevent the computer from sleeping when running!\n",
    "try:\n",
    "    if platform.system() == 'Windows':\n",
    "        # Windows command to prevent sleep\n",
    "        subprocess.Popen(['powercfg', '-change', '-standby-timeout-ac', '0'])\n",
    "        subprocess.Popen(['powercfg', '-change', '-monitor-timeout-ac', '0'])\n",
    "        print(\"Sleep prevention activated for Windows\")\n",
    "    elif platform.system() == 'Darwin':  # macOS\n",
    "        subprocess.Popen(['caffeinate', '-i'])\n",
    "        print(\"Sleep prevention activated for macOS\")\n",
    "    elif platform.system() == 'Linux':\n",
    "        subprocess.Popen(['systemctl', 'mask', 'sleep.target', 'suspend.target', \n",
    "                        'hibernate.target', 'hybrid-sleep.target'])\n",
    "        print(\"Sleep prevention activated for Linux\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not prevent sleep mode: {e}\")\n",
    "\n",
    "# lets run the model and get some responses!\n",
    "model = MedicalGPT()\n",
    "\n",
    "# prepare the data for the required format\n",
    "train_loader, val_loader = model.prepare_separate_datasets(\n",
    "    train_data,\n",
    "    val_data,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# train the model, with a gpu this will take roughly 22 minutes\n",
    "model.train_model(\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_path=\"./medical_gpt2_model\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# test with an arbitrary input\n",
    "question = \"What can I do to lower my blood pressure naturally?\"\n",
    "response = model.generate_medical_response(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 124.44M parameters\n",
      "Generating Medical Responses:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model!\n",
    "model = MedicalGPT()\n",
    "model.model = GPT2LMHeadModel.from_pretrained(\"./medical_gpt2_model\").to(model.device)\n",
    "model.tokenizer = GPT2TokenizerFast.from_pretrained(\"./medical_gpt2_model\")\n",
    "\n",
    "# get all testing data results:\n",
    "questions = [point['question'] for point in test_data]\n",
    "responses = [point['response'] for point in test_data]\n",
    "\n",
    "\n",
    "# Generate responses\n",
    "print(\"Generating Medical Responses:\\n\")\n",
    "with open('gpt2_fine_tuned_results.txt', \"w\", encoding='utf-8') as f:\n",
    "    for question,response in zip(questions, responses):\n",
    "        model_response = model.generate_response(question)\n",
    "        f.write(f\"Question: {model_response}\")\n",
    "        f.write(f\"\\nExpected Response: {response}\\n\")\n",
    "        # response seperator\n",
    "        f.write(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
